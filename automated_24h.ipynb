{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e9ffdc",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28556408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "import tsfresh\n",
    "from tsfresh.feature_extraction.settings import ComprehensiveFCParameters, EfficientFCParameters, MinimalFCParameters\n",
    "from tsfresh.feature_selection.relevance import calculate_relevance_table\n",
    "from tsfresh.transformers import RelevantFeatureAugmenter, FeatureAugmenter, FeatureSelector\n",
    "\n",
    "from utils import Dataset, variance_thresholding, standardize, mcc, calculate_metrics, calculate_metrics_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9962fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for saving data\n",
    "PROCESSED_DATA_DIR = \"processed_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed485f",
   "metadata": {},
   "source": [
    "# Automated feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de33c34",
   "metadata": {},
   "source": [
    "## Utilities and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cb6ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_data_cleaning(data: List[pd.DataFrame]) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Assumes DataFrames with \"timestamp\", \"date\" and \"activity\" columns.\n",
    "    \n",
    "    Performs cleaning operations:\n",
    "    - assure format YYYY-MM-DD HH:MM:SS for \"timestamp\"\n",
    "    - drop redundant \"date\" column\n",
    "    - assure float32 format for \"activity\"\n",
    "    \n",
    "    :param data: list of DataFrames\n",
    "    :returns: list of cleaned DataFrames\n",
    "    \"\"\"\n",
    "    data = [df.copy() for df in data]  # create copy to avoid side effects\n",
    "    \n",
    "    for df in data:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],\n",
    "                                         format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        df.drop(\"date\", axis=1, inplace=True)\n",
    "        df[\"activity\"] = df[\"activity\"].astype(np.float32)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_day_part(df: pd.DataFrame, part: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For given DataFrame with \"timestamp\" column returns only those rows that\n",
    "    correspond to the chosen part of day.\n",
    "    \n",
    "    Parts are \"day\" and \"night\", defined as:\n",
    "    - \"day\": [8:00, 21:00)\n",
    "    - \"night\": [21:00, 8:00)\n",
    "    \n",
    "    :param df: DataFrame to select rows from\n",
    "    :param part: part of day, either \"day\" or \"night\"\n",
    "    :returns: DataFrame, subset of rows of df\n",
    "    \"\"\"\n",
    "    if part == \"day\":\n",
    "        df = df.loc[(df[\"timestamp\"].dt.hour >= 8) &\n",
    "                    (df[\"timestamp\"].dt.hour < 21)]\n",
    "    elif part == \"night\":\n",
    "        df = df.loc[(df[\"timestamp\"].dt.hour >= 21) |\n",
    "                    (df[\"timestamp\"].dt.hour < 8)]\n",
    "    else:\n",
    "        raise ValueError(f'Part should be \"day\" or \"night\", got \"{part}\"')\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_missing_activity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Makes sure that \"timestamp\" column has minute resolution with no missing\n",
    "    values from start to end and replaces all NaNs in \"activity\" column with\n",
    "    mean average value.\n",
    "    \n",
    "    :param data: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    :returns: cleaned DataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()  # create copy to avoid side effects\n",
    "    \n",
    "    # resample to the basic frequency, i.e. minute; this will create NaNs for\n",
    "    # any rows that may be missing\n",
    "    df = df.resample(\"min\", on=\"timestamp\").mean()\n",
    "    \n",
    "    # recreate index and \"timestamp\" column\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # fill any NaNs with mean activity value\n",
    "    df[\"activity\"] = df[\"activity\"].fillna(df[\"activity\"].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def resample(df: pd.DataFrame, freq: str = \"H\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resamples time series DataFrame with given frequency, aggregating each\n",
    "    segment with a mean.\n",
    "\n",
    "    :param df: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    :param freq: resampling frequency passed to Pandas resample() function\n",
    "    :returns: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    \"\"\"\n",
    "    df = df.copy()  # create copy to avoid side effects\n",
    "    \n",
    "    # make sure that data has minute resolution with no missing parts from\n",
    "    # start to end, with no missing values\n",
    "    df = fill_missing_activity(df)\n",
    "    \n",
    "    # group with given frequency\n",
    "    df = df.resample(freq, on=\"timestamp\").mean()\n",
    "\n",
    "    # recreate \"timestamp\" column\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_clean_dataframes(dfs: List[pd.DataFrame], freq: str = \"H\") \\\n",
    "        -> Dict[str, List[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Cleans DataFrames, filling missing values and resampling with given\n",
    "    frequency.\n",
    "    \n",
    "    Returns three lists of DataFrames:\n",
    "    - full 24hs\n",
    "    - days: [8:00, 21:00)\n",
    "    - nights: [21:00, 8:00)\n",
    "    \n",
    "    :param dfs: list of DataFrames to clean; each one has to have \"timestamp\"\n",
    "    and \"activity\" columns\n",
    "    :param freq: resampling frequency\n",
    "    :returns: dictionary with keys \"full_24h\", \"day\" and \"night\", corresponding\n",
    "    to data from given parts of day\n",
    "    \"\"\"\n",
    "    full_dfs = basic_data_cleaning(dfs)\n",
    "    full_dfs = [fill_missing_activity(df) for df in full_dfs]\n",
    "    full_dfs = [resample(df, freq=freq) for df in full_dfs]\n",
    "    \n",
    "    night_dfs = [get_day_part(df, part=\"night\") for df in full_dfs]\n",
    "    day_dfs = [get_day_part(df, part=\"day\") for df in full_dfs]\n",
    "\n",
    "    datasets = {\n",
    "        \"full_24h\": full_dfs,\n",
    "        \"night\": night_dfs,\n",
    "        \"day\": day_dfs\n",
    "    }\n",
    "\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def get_tsfresh_flat_format_df(dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates DataFrame in a \"flat\" format for tsfresh from list of DataFrames.\n",
    "    Each one is assumed to have \"timestamp\" and \"activity\" columns.\n",
    "    \n",
    "    :param dfs: list of DataFrames; each one has to have \"timestamp\" and\n",
    "    \"activity\" columns\n",
    "    :returns: DataFrame in tsfresh \"flat\" format\n",
    "    \"\"\"\n",
    "    dfs = deepcopy(dfs)  # create copy to avoid side effects\n",
    "    \n",
    "    flat_df = pd.DataFrame(columns=[\"id\", \"timestamp\", \"activity\"])\n",
    "\n",
    "    for idx, df in enumerate(dfs):\n",
    "        df[\"id\"] = idx\n",
    "        flat_df = pd.concat([flat_df, df], ignore_index=True) #flat_df.append(df) - stara warsja biblioteki\n",
    "\n",
    "    flat_df = flat_df.reset_index(drop=True)\n",
    "        \n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f3699e",
   "metadata": {},
   "source": [
    "## Parameters and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ac2f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LR\": LogisticRegression(\n",
    "        penalty=\"elasticnet\",\n",
    "        random_state=0,\n",
    "        solver=\"saga\",\n",
    "        max_iter=5000\n",
    "    ),\n",
    "    \"SVM\": SVC(\n",
    "        kernel=\"rbf\",\n",
    "        cache_size=512\n",
    "    ),\n",
    "    \"RF\": RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        criterion=\"entropy\"\n",
    "    ),\n",
    "    \"LGBM\": LGBMClassifier(\n",
    "        n_estimators=500,\n",
    "        verbosity=-1,\n",
    "        random_state=0\n",
    "    ),\n",
    "    \"XGB\": XGBClassifier(\n",
    "        n_estimators=500,\n",
    "        random_state=0 \n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "param_grids = {\n",
    "    \"LR\": {\n",
    "        \"C\": [0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10, 25, 50, 100, 500, 1000],\n",
    "        \"class_weight\": [None, \"balanced\"],\n",
    "        \"l1_ratio\": [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\n",
    "                     0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": np.logspace(10e-3, 10e3, num=50),\n",
    "        \"gamma\": np.logspace(10e-3, 10e3, num=50),\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \"RF\": {\n",
    "        \"class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "    },\n",
    "    \"LGBM\": {\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \"XGB\": {\n",
    "        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b58635",
   "metadata": {},
   "source": [
    "## tsfresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061d193",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89dab571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tsfresh_features(dfs: List[pd.DataFrame], settings: Dict) \\\n",
    "        -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs feature extraction (only extraction, not selection) using tsfresh.\n",
    "    \n",
    "    :param dfs: list of DataFrames with time series, each with \"timestamp\" and\n",
    "    \"activity\" columns\n",
    "    :param settings: tsfresh settings, one of: ComprehensiveFCParameters,\n",
    "    EfficientFCParameters, MinimalFCParameters\n",
    "    :returns: DataFrame with extracted features, with one row per original\n",
    "    DataFrame with time series (in the same order)\n",
    "    \"\"\"\n",
    "    ts = get_tsfresh_flat_format_df(dfs)\n",
    "    ids = ts[\"id\"].unique()\n",
    "    X = pd.DataFrame(index=ids)\n",
    "    \n",
    "    augmenter = FeatureAugmenter(\n",
    "        default_fc_parameters=settings,\n",
    "        column_id=\"id\",\n",
    "        column_sort=\"timestamp\",\n",
    "        column_value=\"activity\",\n",
    "        chunksize=1,\n",
    "        n_jobs=4\n",
    "    )\n",
    "    \n",
    "    augmenter.set_timeseries_container(ts)\n",
    "    X = augmenter.transform(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "class IncreasingFDRFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Selects features using tsfresh feature selector and increasing FDR, if no\n",
    "    features are selected at default FDR=0.05.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose: bool = False):\n",
    "        self.selector: FeatureSelector = None\n",
    "        self.verbose: bool = verbose\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        final_alpha = None\n",
    "        for alpha in [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\n",
    "                      0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]:\n",
    "            self.selector = FeatureSelector(\n",
    "                fdr_level=alpha,\n",
    "                n_jobs=4,\n",
    "                chunksize=1\n",
    "            )\n",
    "            self.selector.fit(X, y)\n",
    "            if len(self.selector.relevant_features) > 0:\n",
    "                if self.verbose:\n",
    "                    print(\"FDR:\", final_alpha)\n",
    "                return selector\n",
    "\n",
    "        raise ValueError(\"Failed to select any features\")\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "\n",
    "class TsfreshTopNFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Selects top N features using tsfresh feature selector.\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int = 10):\n",
    "        self.n: int = n\n",
    "        self.features: List[int] = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        \n",
    "        if not isinstance(y, pd.Series):\n",
    "            y = pd.Series(y)\n",
    "        \n",
    "        relevance_table = calculate_relevance_table(X, y)\n",
    "        relevance_table.sort_values(\"p_value\", inplace=True)\n",
    "        features = relevance_table.head(self.n)[\"feature\"]\n",
    "        self.features = list(features.values)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X[:, self.features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374bcdda-ac60-4e8b-ae29-8dbb1d5b98db",
   "metadata": {},
   "source": [
    "## Psykose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c1666",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0fdb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(dirpath=os.path.join(\"data_24h\", \"psykose\"))\n",
    "condition = dataset.condition\n",
    "control = dataset.control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a1ede3f-a4dc-4eae-92d3-c89ef558d5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "      <th>activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-09-02 09:00:00</td>\n",
       "      <td>2003-09-02</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-09-02 09:01:00</td>\n",
       "      <td>2003-09-02</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-09-02 09:02:00</td>\n",
       "      <td>2003-09-02</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-09-02 09:03:00</td>\n",
       "      <td>2003-09-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-09-02 09:04:00</td>\n",
       "      <td>2003-09-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1435</th>\n",
       "      <td>2003-09-03 08:55:00</td>\n",
       "      <td>2003-09-03</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>2003-09-03 08:56:00</td>\n",
       "      <td>2003-09-03</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>2003-09-03 08:57:00</td>\n",
       "      <td>2003-09-03</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>2003-09-03 08:58:00</td>\n",
       "      <td>2003-09-03</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>2003-09-03 08:59:00</td>\n",
       "      <td>2003-09-03</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp        date  activity\n",
       "0     2003-09-02 09:00:00  2003-09-02       665\n",
       "1     2003-09-02 09:01:00  2003-09-02       166\n",
       "2     2003-09-02 09:02:00  2003-09-02       116\n",
       "3     2003-09-02 09:03:00  2003-09-02         0\n",
       "4     2003-09-02 09:04:00  2003-09-02         0\n",
       "...                   ...         ...       ...\n",
       "1435  2003-09-03 08:55:00  2003-09-03       709\n",
       "1436  2003-09-03 08:56:00  2003-09-03       178\n",
       "1437  2003-09-03 08:57:00  2003-09-03       384\n",
       "1438  2003-09-03 08:58:00  2003-09-03       138\n",
       "1439  2003-09-03 08:59:00  2003-09-03       121\n",
       "\n",
       "[1440 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "529f187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_parts_dfs = get_clean_dataframes(condition, freq=\"min\")\n",
    "control_parts_dfs = get_clean_dataframes(control, freq=\"min\")\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for part in [\"full_24h\"]:\n",
    "    condition_dfs_list = condition_parts_dfs[part]\n",
    "    control_dfs_list = control_parts_dfs[part]\n",
    "    \n",
    "    dfs_list = condition_dfs_list + control_dfs_list\n",
    "    datasets[part] = dfs_list\n",
    "\n",
    "y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, f\"{dataset_str}_y.csv\"), header=None, dtype=int)\n",
    "y = y.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82fa99d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bartek\\AppData\\Local\\Temp\\ipykernel_2540\\1517055941.py:145: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  flat_df = pd.concat([flat_df, df], ignore_index=True) #flat_df.append(df) - stara warsja biblioteki\n",
      "Feature Extraction: 100%|██████████| 1060/1060 [00:04<00:00, 237.32it/s]\n",
      "C:\\Users\\Bartek\\AppData\\Local\\Temp\\ipykernel_2540\\1517055941.py:145: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  flat_df = pd.concat([flat_df, df], ignore_index=True) #flat_df.append(df) - stara warsja biblioteki\n",
      "Feature Extraction: 100%|██████████| 1060/1060 [00:57<00:00, 18.48it/s]\n"
     ]
    }
   ],
   "source": [
    "settings_dict = {\"minimal\": MinimalFCParameters(),\n",
    "                 \"efficient\": EfficientFCParameters()}\n",
    "\n",
    "for part, dfs in datasets.items():\n",
    "    for settings_name, settings in settings_dict.items():\n",
    "        X = extract_tsfresh_features(dfs, settings)\n",
    "        filename = f\"automatic_{dataset_str}_{settings_name}_{part}.csv\"\n",
    "        filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "        X.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e590e",
   "metadata": {},
   "source": [
    "### Minimal settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdecaaaf-4087-406b-baab-e7b14c53159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = \"psykose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "426f9460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PART: full_24h\n",
      "(1060, 10)\n",
      "(54,)\n",
      "  LR\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1060, 54]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m folds \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     17\u001b[0m test_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_idx, test_idx \u001b[38;5;129;01min\u001b[39;00m folds\u001b[38;5;241m.\u001b[39msplit(X, y):\n\u001b[0;32m     19\u001b[0m     X_train, X_test \u001b[38;5;241m=\u001b[39m X[train_idx], X[test_idx]\n\u001b[0;32m     20\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m y[train_idx], y[test_idx]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:406\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \n\u001b[0;32m    385\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m        The testing set indices for that split.\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     X, y, groups \u001b[38;5;241m=\u001b[39m indexable(X, y, groups)\n\u001b[0;32m    407\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m check_consistent_length(\u001b[38;5;241m*\u001b[39mresult)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1060, 54]"
     ]
    }
   ],
   "source": [
    "for part in [\"full_24h\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    \n",
    "    filename = f\"automatic_{dataset_str}_minimal_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    X = pd.read_csv(filepath, header=0).fillna(0).values\n",
    "    \n",
    "    y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, f\"{dataset_str}_y.csv\"), header=None, dtype=int)\n",
    "    y = y.values.ravel()\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "\n",
    "    for clf_type in [\"LR\", \"SVM\", \"RF\", \"LGBM\", \"XGB\"]:\n",
    "        print(f\"  {clf_type}\")\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        \n",
    "        test_scores = []\n",
    "        for train_idx, test_idx in folds.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.05)\n",
    "            X_train, X_test = standardize(X_train, X_test)\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                cv=LeaveOneOut()\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            clf = grid_search.best_estimator_\n",
    "            \n",
    "            metrics = calculate_metrics(clf, X_test, y_test)\n",
    "            print(metrics)\n",
    "            test_scores.append(metrics)\n",
    "        \n",
    "        final_scores = calculate_metrics_statistics(test_scores)\n",
    "        \n",
    "        for metric, (mean, stddev) in final_scores.items():\n",
    "            print(f\"    {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a126d2e",
   "metadata": {},
   "source": [
    "### Efficient settings, increasing FDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = \"depresjon\"  # \"depresjon\" or \"psykose\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0c8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    \n",
    "    filename = f\"automatic_{dataset_str}_efficient_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    X = pd.read_csv(filepath, header=0).fillna(0).values\n",
    "    \n",
    "    y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, f\"{dataset_str}_y.csv\"), header=None, dtype=int)\n",
    "    y = y.values.ravel()\n",
    "\n",
    "    for clf_type in [\"LR\", \"SVM\", \"RF\"]:\n",
    "        print(f\"  {clf_type}\")\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        \n",
    "        test_scores = []\n",
    "        for train_idx, test_idx in folds.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.05)\n",
    "            \n",
    "            selector = IncreasingFDRFeatureSelector(verbose=True)\n",
    "            selector.fit(X_train, y_train)\n",
    "            X_train, X_test = selector.transform(X_train), selector.transform(X_test)\n",
    "            \n",
    "            X_train, X_test = standardize(X_train, X_test)\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                cv=LeaveOneOut()\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            clf = grid_search.best_estimator_\n",
    "            \n",
    "            metrics = calculate_metrics(clf, X_test, y_test)\n",
    "            print(metrics)\n",
    "            test_scores.append(metrics)\n",
    "        \n",
    "        final_scores = calculate_metrics_statistics(test_scores)\n",
    "        \n",
    "        for metric, (mean, stddev) in final_scores.items():\n",
    "            print(f\"    {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256edb7",
   "metadata": {},
   "source": [
    "### Efficient settings, top N features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fd2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = \"depresjon\"  # \"depresjon\" or \"psykose\"\n",
    "\n",
    "top_n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91732bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in [\"full_24h\", \"night\", \"day\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    \n",
    "    filename = f\"automatic_{dataset_str}_efficient_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    X = pd.read_csv(filepath, header=0).fillna(0).values\n",
    "    \n",
    "    y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, f\"{dataset_str}_y.csv\"), header=None, dtype=int)\n",
    "    y = y.values.ravel()\n",
    "\n",
    "    for clf_type in [\"LR\", \"SVM\", \"RF\"]:\n",
    "        print(f\"  {clf_type}\")\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        \n",
    "        test_scores = []\n",
    "        for train_idx, test_idx in folds.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.05)\n",
    "\n",
    "            selector = TsfreshTopNFeatureSelector(n=top_n)\n",
    "            selector.fit(X_train, y_train)\n",
    "            X_train, X_test = selector.transform(X_train), selector.transform(X_test)\n",
    "            \n",
    "            X_train, X_test = standardize(X_train, X_test)\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                cv=LeaveOneOut()\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            clf = grid_search.best_estimator_\n",
    "            \n",
    "            metrics = calculate_metrics(clf, X_test, y_test)\n",
    "            print(metrics)\n",
    "            test_scores.append(metrics)\n",
    "        \n",
    "        final_scores = calculate_metrics_statistics(test_scores)\n",
    "        \n",
    "        for metric, (mean, stddev) in final_scores.items():\n",
    "            print(f\"    {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd5a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
