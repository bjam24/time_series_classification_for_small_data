{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320837f7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18e18bae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:07.838810Z",
     "start_time": "2025-01-12T21:08:06.472162Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import scipy.stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, LeaveOneOut\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from utils import Dataset, variance_thresholding, standardize, mcc, calculate_metrics, calculate_metrics_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b84c8514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:07.849215Z",
     "start_time": "2025-01-12T21:08:07.841418Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameters for Welch's method for estimating power spectrum\n",
    "\n",
    "NPERSEG = 11                    # length of segment\n",
    "NOVERLAP = int(0.75 * NPERSEG)  # overlap of segments\n",
    "NFFT = NPERSEG                  # length of FFT\n",
    "WINDOW = \"hann\"                 # window function type\n",
    "\n",
    "# parameters for saving data\n",
    "PROCESSED_DATA_DIR = \"processed_data24h\"\n",
    "DEPRESJON_PREFIX = \"manual_depresjon24h\"\n",
    "HYPERAKTIV_PREFIX = \"manual_hyperaktiv24h\"\n",
    "PSYKOSE_PREFIX = \"manual_psykose24h\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef0d95",
   "metadata": {},
   "source": [
    "# Manual feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9515d10",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657c3d7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:07.886366Z",
     "start_time": "2025-01-12T21:08:07.876013Z"
    }
   },
   "outputs": [],
   "source": [
    "def basic_data_cleaning(data: List[pd.DataFrame]) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Assumes DataFrames with \"timestamp\", \"date\" and \"activity\" columns.\n",
    "    \n",
    "    Performs cleaning operations:\n",
    "    - assure format YYYY-MM-DD HH:MM:SS for \"timestamp\"\n",
    "    - drop redundant \"date\" column\n",
    "    - assure float32 format for \"activity\"\n",
    "    \n",
    "    :param data: list of DataFrames\n",
    "    :returns: list of cleaned DataFrames\n",
    "    \"\"\"\n",
    "    data = [df.copy() for df in data]  # create copy to avoid side effects\n",
    "    \n",
    "    for df in data:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],\n",
    "                                         format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        df.drop(\"date\", axis=1, inplace=True)\n",
    "        df[\"activity\"] = df[\"activity\"].astype(np.float32)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_day_part(df: pd.DataFrame, part: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For given DataFrame with \"timestamp\" column returns only those rows that\n",
    "    correspond to the chosen part of day.\n",
    "    \n",
    "    Parts are \"day\" and \"night\", defined as:\n",
    "    - \"day\": [8:00, 21:00)\n",
    "    - \"night\": [21:00, 8:00)\n",
    "    \n",
    "    :param df: DataFrame to select rows from\n",
    "    :param part: part of day, either \"day\" or \"night\"\n",
    "    :returns: DataFrame, subset of rows of df\n",
    "    \"\"\"\n",
    "    if part == \"day\":\n",
    "        df = df.loc[(df[\"timestamp\"].dt.hour >= 8) &\n",
    "                    (df[\"timestamp\"].dt.hour < 21)]\n",
    "    elif part == \"night\":\n",
    "        df = df.loc[(df[\"timestamp\"].dt.hour >= 21) |\n",
    "                    (df[\"timestamp\"].dt.hour < 8)]\n",
    "    else:\n",
    "        raise ValueError(f'Part should be \"day\" or \"night\", got \"{part}\"')\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_missing_activity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Makes sure that \"timestamp\" column has minute resolution with no missing\n",
    "    values from start to end and replaces all NaNs in \"activity\" column with\n",
    "    mean average value.\n",
    "    \n",
    "    :param data: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    :returns: cleaned DataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()  # create copy to avoid side effects\n",
    "    \n",
    "    # resample to the basic frequency, i.e. minute; this will create NaNs for\n",
    "    # any rows that may be missing\n",
    "    df = df.resample(\"min\", on=\"timestamp\").mean()\n",
    "    \n",
    "    # recreate index and \"timestamp\" column\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # fill any NaNs with mean activity value\n",
    "    df[\"activity\"] = df[\"activity\"].fillna(df[\"activity\"].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def resample(df: pd.DataFrame, freq: str = \"H\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resamples time series DataFrame with given frequency, aggregating each\n",
    "    segment with a mean.\n",
    "\n",
    "    :param df: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    :param freq: resampling frequency passed to Pandas resample() function\n",
    "    :returns: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    \"\"\"\n",
    "    df = df.copy()  # create copy to avoid side effects\n",
    "    \n",
    "    # group with given frequency\n",
    "    df = df.resample(freq, on=\"timestamp\").mean()\n",
    "\n",
    "    # recreate \"timestamp\" column\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def proportion_of_zeros(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates proportion of zeros in given array, i.e. number of zeros divided\n",
    "    by length of array.\n",
    "    \n",
    "    :param x: 1D Numpy array\n",
    "    :returns: proportion of zeros\n",
    "    \"\"\"\n",
    "    # we may be dealing with floating numbers, we can't use direct comparison\n",
    "    zeros_count = np.sum(np.isclose(x, 0))\n",
    "    return zeros_count / len(x)\n",
    "\n",
    "\n",
    "def power_spectral_density(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates power spectral density (PSD) from \"activity\" column of a\n",
    "    DataFrame.\n",
    "    \n",
    "    :param df: DataFrame with \"activity\" column\n",
    "    :returns: 1D Numpy array with power spectral density\n",
    "    \"\"\"\n",
    "    psd = scipy.signal.welch(\n",
    "        x=df[\"activity\"].values,\n",
    "        fs=(1/11),\n",
    "        nperseg=11,\n",
    "        noverlap=10,\n",
    "        nfft=NFFT,\n",
    "        window=WINDOW,\n",
    "        scaling=\"density\"\n",
    "    )[1]\n",
    "    return psd\n",
    "\n",
    "\n",
    "def spectral_flatness(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculates spectral flatness of a signal, i.e. a geometric mean of the\n",
    "    power spectrum divided by the arithmetic mean of the power spectrum.\n",
    "    \n",
    "    If some frequency bins in the power spectrum are close to zero, they are\n",
    "    removed prior to calculation of spectral flatness to avoid calculation of\n",
    "    log(0).\n",
    "    \n",
    "    :param df: DataFrame with \"activity\" column\n",
    "    :returns: spectral flatness value\n",
    "    \"\"\"\n",
    "    power_spectrum = scipy.signal.welch(\n",
    "        df[\"activity\"].values,\n",
    "        fs=(1/11),\n",
    "        nperseg=11,\n",
    "        noverlap=10,\n",
    "        nfft=NFFT,\n",
    "        window=WINDOW,\n",
    "        scaling=\"spectrum\"\n",
    "    )[1]\n",
    "    \n",
    "    non_zeros_mask = ~np.isclose(power_spectrum, 0)\n",
    "    power_spectrum = power_spectrum[non_zeros_mask]\n",
    "    \n",
    "    return scipy.stats.gmean(power_spectrum) / power_spectrum.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c465a53",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16826d4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:07.894452Z",
     "start_time": "2025-01-12T21:08:07.890056Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts features from activity signal in time domain.\n",
    "    \n",
    "    :param df_resampled: DataFrame with \"activity\" column\n",
    "    :returns: DataFrame with a single row representing features\n",
    "    \"\"\"\n",
    "    X = df[\"activity\"].values\n",
    "    \n",
    "    features = {\n",
    "        \"minimum\": np.min(X),\n",
    "        \"maximum\": np.max(X),\n",
    "        \"mean\": np.mean(X),\n",
    "        \"median\": np.median(X),\n",
    "        \"variance\": np.var(X, ddof=1),  # apply Bessel's correction\n",
    "        \"kurtosis\": sp.stats.kurtosis(X),\n",
    "        \"skewness\": sp.stats.skew(X),\n",
    "        \"coeff_of_var\": sp.stats.variation(X),\n",
    "        \"iqr\": sp.stats.iqr(X),\n",
    "        \"trimmed_mean\": sp.stats.trim_mean(X, proportiontocut=0.1),\n",
    "        \"entropy\": sp.stats.entropy(X, base=2),\n",
    "        \"proportion_of_zeros\": proportion_of_zeros(X)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9589aaf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:07.900589Z",
     "start_time": "2025-01-12T21:08:07.896052Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_frequency_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts features from activity signal in frequency domain, i.e. calculated\n",
    "    from its Power Spectral Density (PSD).\n",
    "    \n",
    "    :param df: DataFrame with \"activity\" column\n",
    "    :returns: DataFrame with a single row representing features\n",
    "    \"\"\"\n",
    "    X = power_spectral_density(df)\n",
    "    \n",
    "    features = {\n",
    "        \"minimum\": np.min(X),\n",
    "        \"maximum\": np.max(X),\n",
    "        \"mean\": np.mean(X),\n",
    "        \"median\": np.median(X),\n",
    "        \"variance\": np.var(X),\n",
    "        \"kurtosis\": sp.stats.kurtosis(X),\n",
    "        \"skewness\": sp.stats.skew(X),\n",
    "        \"coeff_of_var\": sp.stats.variation(X),\n",
    "        \"iqr\": sp.stats.iqr(X),\n",
    "        \"trimmed_mean\": sp.stats.trim_mean(X, proportiontocut=0.1),\n",
    "        \"entropy\": sp.stats.entropy(X, base=2),\n",
    "        \"spectral_flatness\": spectral_flatness(df)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9867a83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:07.906109Z",
     "start_time": "2025-01-12T21:08:07.902126Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features_for_dataframes(dfs: List[pd.DataFrame], freq: str = \"H\") \\\n",
    "        -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Calculates time and frequency features for given DataFrames. Uses given\n",
    "    frequency for resampling.\n",
    "    \n",
    "    Calculates features separately for:\n",
    "    - nights: [21:00, 8:00)\n",
    "    \n",
    "    :param dfs: list of DataFrames to extract features from; each one has to\n",
    "    have \"timestamp\" and \"activity\" columns\n",
    "    :param freq: resampling frequency\n",
    "    :returns: dictionary with keys \"night\", corresponding\n",
    "    to features from given parts of day\n",
    "    \"\"\"\n",
    "    full_dfs = basic_data_cleaning(dfs)\n",
    "    full_dfs = [fill_missing_activity(df) for df in full_dfs]\n",
    "    full_dfs = [resample(df, freq=freq) for df in full_dfs]\n",
    "\n",
    "    datasets = {}\n",
    "    \n",
    "    for part, list_of_dfs in [(\"night\", full_dfs)]:\n",
    "        features = []\n",
    "        for df in list_of_dfs:\n",
    "            time_features = extract_time_features(df)\n",
    "            freq_features = extract_frequency_features(df)\n",
    "\n",
    "            merged_features = pd.merge(\n",
    "                time_features,\n",
    "                freq_features,\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                suffixes=[\"_time\", \"_freq\"]\n",
    "            )\n",
    "            features.append(merged_features)\n",
    "\n",
    "        datasets[part] = pd.concat(features)\n",
    "        datasets[part].reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6d128",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Depresjon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "278e4de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:08.573744Z",
     "start_time": "2025-01-12T21:08:07.907305Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(dirpath=os.path.join(\"data_night\", \"depresjon\"))\n",
    "condition = dataset.condition\n",
    "control = dataset.control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0234b1a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:08.582253Z",
     "start_time": "2025-01-12T21:08:08.575011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "               timestamp        date  activity\n0    2003-05-07 21:00:00  2003-05-07       212\n1    2003-05-07 21:01:00  2003-05-07       212\n2    2003-05-07 21:02:00  2003-05-07        17\n3    2003-05-07 21:03:00  2003-05-07        38\n4    2003-05-07 21:04:00  2003-05-07        82\n..                   ...         ...       ...\n655  2003-05-08 07:55:00  2003-05-08         9\n656  2003-05-08 07:56:00  2003-05-08        23\n657  2003-05-08 07:57:00  2003-05-08         0\n658  2003-05-08 07:58:00  2003-05-08         0\n659  2003-05-08 07:59:00  2003-05-08        20\n\n[660 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>date</th>\n      <th>activity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003-05-07 21:00:00</td>\n      <td>2003-05-07</td>\n      <td>212</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003-05-07 21:01:00</td>\n      <td>2003-05-07</td>\n      <td>212</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003-05-07 21:02:00</td>\n      <td>2003-05-07</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003-05-07 21:03:00</td>\n      <td>2003-05-07</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003-05-07 21:04:00</td>\n      <td>2003-05-07</td>\n      <td>82</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>655</th>\n      <td>2003-05-08 07:55:00</td>\n      <td>2003-05-08</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>656</th>\n      <td>2003-05-08 07:56:00</td>\n      <td>2003-05-08</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>657</th>\n      <td>2003-05-08 07:57:00</td>\n      <td>2003-05-08</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>658</th>\n      <td>2003-05-08 07:58:00</td>\n      <td>2003-05-08</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>659</th>\n      <td>2003-05-08 07:59:00</td>\n      <td>2003-05-08</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n<p>660 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb28b3e0",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:16.363171Z",
     "start_time": "2025-01-12T21:08:08.583426Z"
    }
   },
   "outputs": [],
   "source": [
    "condition_parts_dfs = extract_features_for_dataframes(condition, freq=\"H\")\n",
    "control_parts_dfs = extract_features_for_dataframes(control, freq=\"H\")\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for part in [\"night\"]:\n",
    "    condition_df = condition_parts_dfs[part]\n",
    "    control_df = control_parts_dfs[part]\n",
    "    \n",
    "    entire_df = pd.concat([condition_df, control_df], ignore_index=True)\n",
    "    \n",
    "    # Przypisujemy wynik do słownika datasets\n",
    "    datasets[part] = entire_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdbfbb10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:16.523405Z",
     "start_time": "2025-01-12T21:08:16.507560Z"
    }
   },
   "outputs": [],
   "source": [
    "for part, df in datasets.items():\n",
    "    filename = f\"{DEPRESJON_PREFIX}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     minimum_time  maximum_time  mean_time  median_time  variance_time  \\\n0        0.966667    148.733337  25.707579    10.000000    1821.470337   \n1        2.083333    120.050003  33.522724    12.650000    1455.296509   \n2        0.583333    178.116669  46.118183    17.783333    3332.064453   \n3        0.916667     85.483330  25.483332    10.866667     968.310364   \n4        2.666667    111.233330  24.503029     9.816667    1099.974243   \n..            ...           ...        ...          ...            ...   \n377      0.000000     16.533333   2.636364     0.000000      27.489933   \n378      0.000000      1.850000   0.313636     0.000000       0.474545   \n379      0.000000      1.200000   0.160606     0.000000       0.147737   \n380      0.000000      1.350000   0.228788     0.000000       0.177838   \n381      0.000000      0.250000   0.037879     0.000000       0.007449   \n\n     kurtosis_time  skewness_time  coeff_of_var_time   iqr_time  \\\n0         4.635292       2.443026           1.582900  17.500001   \n1         0.244883       1.169350           1.085026  51.141668   \n2         0.344395       1.195673           1.193406  74.924999   \n3        -0.284877       1.166769           1.164273  28.716667   \n4         2.282830       1.835684           1.290551  20.741667   \n..             ...            ...                ...        ...   \n377       2.537389       1.916340           1.896204   2.925000   \n378       0.903060       1.684765           2.094191   0.025000   \n379       3.049299       2.128762           2.281849   0.000000   \n380       2.642057       1.935837           1.757452   0.275000   \n381       1.595087       1.825546           2.172556   0.000000   \n\n     trimmed_mean_time  ...     mean_freq   median_freq  variance_freq  \\\n0            14.787038  ...  13916.573242  10138.624023   9.799489e+07   \n1            27.401852  ...  13156.655273   7906.802734   2.379827e+08   \n2            36.511112  ...  39708.550781  24891.986328   1.254300e+09   \n3            21.546295  ...   8917.659180   6521.940918   5.759565e+07   \n4            17.292593  ...   9094.897461   6941.023438   4.072176e+07   \n..                 ...  ...           ...           ...            ...   \n377           1.385185  ...    143.984879     22.406513   3.182891e+04   \n378           0.177778  ...      2.927676      1.562682   5.071979e+00   \n379           0.062963  ...      1.182480      0.732170   1.642501e+00   \n380           0.129630  ...      1.330155      1.300855   9.277220e-01   \n381           0.018519  ...      0.029402      0.000933   1.697620e-03   \n\n     kurtosis_freq  skewness_freq  coeff_of_var_freq      iqr_freq  \\\n0         1.124900       1.744361           0.711327   1125.172607   \n1         0.439530       1.376406           1.172539  11228.363770   \n2        -0.393184       0.991210           0.891901  37194.058105   \n3        -0.566328       0.864708           0.851028   9563.873108   \n4         0.514084       1.383717           0.701642   3578.424805   \n..             ...            ...                ...           ...   \n377      -1.497008       0.706503           1.239065    283.116976   \n378       0.299998       1.385461           0.769247      1.630936   \n379      -0.137021       1.096713           1.083825      1.356251   \n380      -1.363149       0.268926           0.724114      1.474317   \n381      -1.108645       0.847050           1.401346      0.052677   \n\n     trimmed_mean_freq  entropy_freq  spectral_flatness  \n0         13916.574219      2.302669           0.847654  \n1         13156.653320      1.720446           0.405417  \n2         39708.550781      2.048502           0.632379  \n3          8917.659180      2.086632           0.667920  \n4          9094.897461      2.283462           0.816446  \n..                 ...           ...                ...  \n377         143.984879      1.490054           0.339778  \n378           2.927676      2.233446           0.799678  \n379           1.182480      1.780805           0.430636  \n380           1.330155      2.171833           0.681011  \n381           0.029402      1.147174           0.142819  \n\n[382 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>minimum_time</th>\n      <th>maximum_time</th>\n      <th>mean_time</th>\n      <th>median_time</th>\n      <th>variance_time</th>\n      <th>kurtosis_time</th>\n      <th>skewness_time</th>\n      <th>coeff_of_var_time</th>\n      <th>iqr_time</th>\n      <th>trimmed_mean_time</th>\n      <th>...</th>\n      <th>mean_freq</th>\n      <th>median_freq</th>\n      <th>variance_freq</th>\n      <th>kurtosis_freq</th>\n      <th>skewness_freq</th>\n      <th>coeff_of_var_freq</th>\n      <th>iqr_freq</th>\n      <th>trimmed_mean_freq</th>\n      <th>entropy_freq</th>\n      <th>spectral_flatness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.966667</td>\n      <td>148.733337</td>\n      <td>25.707579</td>\n      <td>10.000000</td>\n      <td>1821.470337</td>\n      <td>4.635292</td>\n      <td>2.443026</td>\n      <td>1.582900</td>\n      <td>17.500001</td>\n      <td>14.787038</td>\n      <td>...</td>\n      <td>13916.573242</td>\n      <td>10138.624023</td>\n      <td>9.799489e+07</td>\n      <td>1.124900</td>\n      <td>1.744361</td>\n      <td>0.711327</td>\n      <td>1125.172607</td>\n      <td>13916.574219</td>\n      <td>2.302669</td>\n      <td>0.847654</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.083333</td>\n      <td>120.050003</td>\n      <td>33.522724</td>\n      <td>12.650000</td>\n      <td>1455.296509</td>\n      <td>0.244883</td>\n      <td>1.169350</td>\n      <td>1.085026</td>\n      <td>51.141668</td>\n      <td>27.401852</td>\n      <td>...</td>\n      <td>13156.655273</td>\n      <td>7906.802734</td>\n      <td>2.379827e+08</td>\n      <td>0.439530</td>\n      <td>1.376406</td>\n      <td>1.172539</td>\n      <td>11228.363770</td>\n      <td>13156.653320</td>\n      <td>1.720446</td>\n      <td>0.405417</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.583333</td>\n      <td>178.116669</td>\n      <td>46.118183</td>\n      <td>17.783333</td>\n      <td>3332.064453</td>\n      <td>0.344395</td>\n      <td>1.195673</td>\n      <td>1.193406</td>\n      <td>74.924999</td>\n      <td>36.511112</td>\n      <td>...</td>\n      <td>39708.550781</td>\n      <td>24891.986328</td>\n      <td>1.254300e+09</td>\n      <td>-0.393184</td>\n      <td>0.991210</td>\n      <td>0.891901</td>\n      <td>37194.058105</td>\n      <td>39708.550781</td>\n      <td>2.048502</td>\n      <td>0.632379</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.916667</td>\n      <td>85.483330</td>\n      <td>25.483332</td>\n      <td>10.866667</td>\n      <td>968.310364</td>\n      <td>-0.284877</td>\n      <td>1.166769</td>\n      <td>1.164273</td>\n      <td>28.716667</td>\n      <td>21.546295</td>\n      <td>...</td>\n      <td>8917.659180</td>\n      <td>6521.940918</td>\n      <td>5.759565e+07</td>\n      <td>-0.566328</td>\n      <td>0.864708</td>\n      <td>0.851028</td>\n      <td>9563.873108</td>\n      <td>8917.659180</td>\n      <td>2.086632</td>\n      <td>0.667920</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.666667</td>\n      <td>111.233330</td>\n      <td>24.503029</td>\n      <td>9.816667</td>\n      <td>1099.974243</td>\n      <td>2.282830</td>\n      <td>1.835684</td>\n      <td>1.290551</td>\n      <td>20.741667</td>\n      <td>17.292593</td>\n      <td>...</td>\n      <td>9094.897461</td>\n      <td>6941.023438</td>\n      <td>4.072176e+07</td>\n      <td>0.514084</td>\n      <td>1.383717</td>\n      <td>0.701642</td>\n      <td>3578.424805</td>\n      <td>9094.897461</td>\n      <td>2.283462</td>\n      <td>0.816446</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>377</th>\n      <td>0.000000</td>\n      <td>16.533333</td>\n      <td>2.636364</td>\n      <td>0.000000</td>\n      <td>27.489933</td>\n      <td>2.537389</td>\n      <td>1.916340</td>\n      <td>1.896204</td>\n      <td>2.925000</td>\n      <td>1.385185</td>\n      <td>...</td>\n      <td>143.984879</td>\n      <td>22.406513</td>\n      <td>3.182891e+04</td>\n      <td>-1.497008</td>\n      <td>0.706503</td>\n      <td>1.239065</td>\n      <td>283.116976</td>\n      <td>143.984879</td>\n      <td>1.490054</td>\n      <td>0.339778</td>\n    </tr>\n    <tr>\n      <th>378</th>\n      <td>0.000000</td>\n      <td>1.850000</td>\n      <td>0.313636</td>\n      <td>0.000000</td>\n      <td>0.474545</td>\n      <td>0.903060</td>\n      <td>1.684765</td>\n      <td>2.094191</td>\n      <td>0.025000</td>\n      <td>0.177778</td>\n      <td>...</td>\n      <td>2.927676</td>\n      <td>1.562682</td>\n      <td>5.071979e+00</td>\n      <td>0.299998</td>\n      <td>1.385461</td>\n      <td>0.769247</td>\n      <td>1.630936</td>\n      <td>2.927676</td>\n      <td>2.233446</td>\n      <td>0.799678</td>\n    </tr>\n    <tr>\n      <th>379</th>\n      <td>0.000000</td>\n      <td>1.200000</td>\n      <td>0.160606</td>\n      <td>0.000000</td>\n      <td>0.147737</td>\n      <td>3.049299</td>\n      <td>2.128762</td>\n      <td>2.281849</td>\n      <td>0.000000</td>\n      <td>0.062963</td>\n      <td>...</td>\n      <td>1.182480</td>\n      <td>0.732170</td>\n      <td>1.642501e+00</td>\n      <td>-0.137021</td>\n      <td>1.096713</td>\n      <td>1.083825</td>\n      <td>1.356251</td>\n      <td>1.182480</td>\n      <td>1.780805</td>\n      <td>0.430636</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>0.000000</td>\n      <td>1.350000</td>\n      <td>0.228788</td>\n      <td>0.000000</td>\n      <td>0.177838</td>\n      <td>2.642057</td>\n      <td>1.935837</td>\n      <td>1.757452</td>\n      <td>0.275000</td>\n      <td>0.129630</td>\n      <td>...</td>\n      <td>1.330155</td>\n      <td>1.300855</td>\n      <td>9.277220e-01</td>\n      <td>-1.363149</td>\n      <td>0.268926</td>\n      <td>0.724114</td>\n      <td>1.474317</td>\n      <td>1.330155</td>\n      <td>2.171833</td>\n      <td>0.681011</td>\n    </tr>\n    <tr>\n      <th>381</th>\n      <td>0.000000</td>\n      <td>0.250000</td>\n      <td>0.037879</td>\n      <td>0.000000</td>\n      <td>0.007449</td>\n      <td>1.595087</td>\n      <td>1.825546</td>\n      <td>2.172556</td>\n      <td>0.000000</td>\n      <td>0.018519</td>\n      <td>...</td>\n      <td>0.029402</td>\n      <td>0.000933</td>\n      <td>1.697620e-03</td>\n      <td>-1.108645</td>\n      <td>0.847050</td>\n      <td>1.401346</td>\n      <td>0.052677</td>\n      <td>0.029402</td>\n      <td>1.147174</td>\n      <td>0.142819</td>\n    </tr>\n  </tbody>\n</table>\n<p>382 rows × 24 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:16.539371Z",
     "start_time": "2025-01-12T21:08:16.525077Z"
    }
   },
   "id": "e803b6a2e635670d",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9c20b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:16.545891Z",
     "start_time": "2025-01-12T21:08:16.541338Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np.concatenate((np.ones(len(condition)), np.zeros(len(control))))\n",
    "y = pd.Series(y, dtype=int)\n",
    "\n",
    "filepath = os.path.join(PROCESSED_DATA_DIR, f\"depresjon_y.csv\")\n",
    "y.to_csv(filepath, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9127c366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:16.549865Z",
     "start_time": "2025-01-12T21:08:16.547510Z"
    }
   },
   "outputs": [],
   "source": [
    "# to get ensemble:\n",
    "import os\n",
    "\n",
    "def sort_key(filename):\n",
    "    parts = filename.split(\"_\")\n",
    "    # Wydobycie numerów jako liczb całkowitych\n",
    "    condition_number = int(parts[1])  # Druga część np. \"1\" z \"condition_1\"\n",
    "    segment_number = int(parts[3].split(\".\")[0])  # Trzecia część np. \"1\" z \"segmen1.csv\"\n",
    "    return (condition_number, segment_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Removing NaNs\n",
    "folder_control = './data_night/depresjon/control'\n",
    "folder_condition = './data_night/depresjon/condition'\n",
    "z_control = []\n",
    "z_condition = []\n",
    "for filename in sorted(os.listdir(folder_control), key=sort_key):\n",
    "    z_control.append(filename.split(\"_\")[0] + filename.split(\"_\")[1])\n",
    "    \n",
    "for filename in sorted(os.listdir(folder_condition), key=sort_key):\n",
    "    z_condition.append(filename.split(\"_\")[0] + filename.split(\"_\")[1])\n",
    "    \n",
    "z_control = pd.Series(z_control)\n",
    "z_condition = pd.Series(z_condition)\n",
    "\n",
    "z = pd.concat([z_condition, z_control])\n",
    "z.index = datasets[\"night\"].index\n",
    "\n",
    "mask = datasets[\"night\"].notna().all(axis=1)\n",
    "datasets[\"night\"] = datasets[\"night\"][mask]\n",
    "y = y[mask]\n",
    "z_depresjon = z[mask]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:16.557514Z",
     "start_time": "2025-01-12T21:08:16.551476Z"
    }
   },
   "id": "902f2fb34966c617",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "9e10ba1f",
   "metadata": {},
   "source": [
    "## Psykose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f315198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:17.693835Z",
     "start_time": "2025-01-12T21:08:16.558863Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(dirpath=os.path.join(\"data_night\", \"psykose\"))\n",
    "condition = dataset.condition\n",
    "control = dataset.control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5686d4e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:25.382684Z",
     "start_time": "2025-01-12T21:08:17.696951Z"
    }
   },
   "outputs": [],
   "source": [
    "condition_parts_dfs = extract_features_for_dataframes(condition, freq=\"H\")\n",
    "control_parts_dfs = extract_features_for_dataframes(control, freq=\"H\")\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for part in [\"night\"]:\n",
    "    condition_df = condition_parts_dfs[part]\n",
    "    control_df = control_parts_dfs[part]\n",
    "\n",
    "    entire_df = pd.concat([condition_df, control_df], ignore_index=True)\n",
    "\n",
    "    # Przypisujemy wynik do słownika datasets\n",
    "    datasets[part] = entire_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cc17091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:25.402528Z",
     "start_time": "2025-01-12T21:08:25.386027Z"
    }
   },
   "outputs": [],
   "source": [
    "for part, df in datasets.items():\n",
    "    filename = f\"{PSYKOSE_PREFIX}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e75053c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:25.409777Z",
     "start_time": "2025-01-12T21:08:25.404422Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np.concatenate((np.ones(len(condition)), np.zeros(len(control))))\n",
    "y = pd.Series(y, dtype=int)\n",
    "\n",
    "filepath = os.path.join(PROCESSED_DATA_DIR, f\"psykose_y.csv\")\n",
    "y.to_csv(filepath, header=False, index=False)\n",
    "#datasets[\"full_24h\"].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Removing NaNs\n",
    "folder_control = './data_night/psykose/control'\n",
    "folder_condition = './data_night/psykose/condition'\n",
    "z_control = []\n",
    "z_condition = []\n",
    "for filename in sorted(os.listdir(folder_control), key=sort_key):\n",
    "    z_control.append(filename.split(\"_\")[0] + filename.split(\"_\")[1])\n",
    "    \n",
    "for filename in sorted(os.listdir(folder_condition), key=sort_key):\n",
    "    z_condition.append(filename.split(\"_\")[0] + filename.split(\"_\")[1])\n",
    "    \n",
    "z_control = pd.Series(z_control)\n",
    "z_condition = pd.Series(z_condition)\n",
    "\n",
    "z = pd.concat([z_condition, z_control])\n",
    "z.index = datasets[\"night\"].index\n",
    "mask = datasets[\"night\"].notna().all(axis=1)\n",
    "datasets[\"night\"] = datasets[\"night\"][mask]\n",
    "y = y[mask]\n",
    "z_psykose = z[mask]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:25.418646Z",
     "start_time": "2025-01-12T21:08:25.411604Z"
    }
   },
   "id": "5626befe714bfb4f",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperactive"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a30bcb43f445c55f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = Dataset(dirpath=os.path.join(\"data_night\", \"hyperaktiv\"))\n",
    "condition = dataset.condition\n",
    "control = dataset.control"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:25.821287Z",
     "start_time": "2025-01-12T21:08:25.421148Z"
    }
   },
   "id": "777581d56f0f531a",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "condition_parts_dfs = extract_features_for_dataframes(condition, freq=\"H\")\n",
    "control_parts_dfs = extract_features_for_dataframes(control, freq=\"H\")\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for part in [\"night\"]:\n",
    "    condition_df = condition_parts_dfs[part]\n",
    "    control_df = control_parts_dfs[part]\n",
    "\n",
    "    entire_df = pd.concat([condition_df, control_df], ignore_index=True)\n",
    "\n",
    "    # Przypisujemy wynik do słownika datasets\n",
    "    datasets[part] = entire_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:30.690054Z",
     "start_time": "2025-01-12T21:08:25.822800Z"
    }
   },
   "id": "a7dabd87c6d38ca6",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for part, df in datasets.items():\n",
    "    filename = f\"{HYPERAKTIV_PREFIX}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    df.to_csv(filepath, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:30.704481Z",
     "start_time": "2025-01-12T21:08:30.692363Z"
    }
   },
   "id": "a3d83ce630086a31",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y = np.concatenate((np.ones(len(condition)), np.zeros(len(control))))\n",
    "y = pd.Series(y, dtype=int)\n",
    "\n",
    "filepath = os.path.join(PROCESSED_DATA_DIR, f\"hyperaktiv_y.csv\")\n",
    "y.to_csv(filepath, header=False, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:30.710960Z",
     "start_time": "2025-01-12T21:08:30.706167Z"
    }
   },
   "id": "44ba2c9c28b97730",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     minimum_time  maximum_time  mean_time  median_time  variance_time  \\\n0        6.016667    230.183334  91.416679    65.266670    7912.653320   \n1        0.000000     73.683334  24.281818    16.816668     588.533997   \n2        0.000000    131.483337  42.448486    27.299999    2579.278076   \n3        0.000000    250.916672  57.416664    16.816668    5966.505371   \n4        4.283333    166.449997  42.993935    16.766666    2538.044434   \n..            ...           ...        ...          ...            ...   \n567      6.650000    298.216675  90.040909    61.849998    8973.646484   \n568      5.550000    513.766663  88.033333    41.150002   21160.406250   \n569      5.216667    197.933334  65.021217    36.383335    4138.562500   \n570      5.033333    426.116669  91.619698    28.716667   16389.708984   \n571      5.300000    127.483330  33.943935    22.883333    1269.806396   \n\n     kurtosis_time  skewness_time  coeff_of_var_time    iqr_time  \\\n0        -1.510052       0.450157           0.927768  176.150001   \n1        -0.423892       0.953657           0.952595   30.108334   \n2        -0.948834       0.820242           1.140750   70.308334   \n3         1.531150       1.564291           1.282702   77.891667   \n4         1.227072       1.483795           1.117239   50.774999   \n..             ...            ...                ...         ...   \n567       0.482736       1.356871           1.003109   62.099999   \n568       5.111922       2.553533           1.575500   74.858335   \n569      -0.087868       1.107714           0.943351   59.875000   \n570       2.290914       1.819472           1.332295   97.125001   \n571       2.383691       1.772810           1.000945   31.833332   \n\n     trimmed_mean_time  ...      mean_freq    median_freq  variance_freq  \\\n0            85.487038  ...  116072.601562  109778.625000   2.681830e+09   \n1            21.490740  ...   12793.206055   10470.508789   1.164978e+08   \n2            37.272224  ...   48747.886719   56377.863281   9.152884e+08   \n3            42.296295  ...   37182.328125   37570.835938   4.365628e+08   \n4            33.577774  ...   68386.632812   43126.476562   3.511676e+09   \n..                 ...  ...            ...            ...            ...   \n567          76.175926  ...   59718.839844   26482.294922   4.067366e+09   \n568          49.894444  ...  103515.695312   25824.949219   1.592926e+10   \n569          56.898151  ...   52158.675781   23151.261719   3.426864e+09   \n570          64.074074  ...  111410.710938   11569.268555   2.198547e+10   \n571          26.733334  ...    9402.422852    8435.019531   5.893434e+07   \n\n     kurtosis_freq  skewness_freq  coeff_of_var_freq       iqr_freq  \\\n0        -1.578465       0.134022           0.446155   90220.271484   \n1        -1.695204       0.203371           0.843683   20261.454712   \n2        -1.230018      -0.380039           0.620616   42063.210938   \n3        -1.241227      -0.135190           0.561936   30318.465332   \n4        -1.304017       0.637380           0.866535   89065.209961   \n..             ...            ...                ...            ...   \n567      -1.308131       0.701559           1.067936   98704.467773   \n568      -1.049457       0.854721           1.219247  159958.384766   \n569       0.262236       1.349471           1.122333   44557.569580   \n570      -1.314444       0.771620           1.330886  208656.183105   \n571      -1.156760       0.447605           0.816478   10542.973938   \n\n     trimmed_mean_freq  entropy_freq  spectral_flatness  \n0        116072.593750      2.436998           0.894241  \n1         12793.206055      1.983479           0.438438  \n2         48747.890625      2.208131           0.535190  \n3         37182.332031      2.315747           0.757898  \n4         68386.632812      2.046365           0.625485  \n..                 ...           ...                ...  \n567       59718.839844      1.760489           0.440584  \n568      103515.695312      1.541633           0.284065  \n569       52158.675781      1.827806           0.558175  \n570      111410.726562      1.297107           0.216173  \n571        9402.422852      2.063326           0.582836  \n\n[570 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>minimum_time</th>\n      <th>maximum_time</th>\n      <th>mean_time</th>\n      <th>median_time</th>\n      <th>variance_time</th>\n      <th>kurtosis_time</th>\n      <th>skewness_time</th>\n      <th>coeff_of_var_time</th>\n      <th>iqr_time</th>\n      <th>trimmed_mean_time</th>\n      <th>...</th>\n      <th>mean_freq</th>\n      <th>median_freq</th>\n      <th>variance_freq</th>\n      <th>kurtosis_freq</th>\n      <th>skewness_freq</th>\n      <th>coeff_of_var_freq</th>\n      <th>iqr_freq</th>\n      <th>trimmed_mean_freq</th>\n      <th>entropy_freq</th>\n      <th>spectral_flatness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6.016667</td>\n      <td>230.183334</td>\n      <td>91.416679</td>\n      <td>65.266670</td>\n      <td>7912.653320</td>\n      <td>-1.510052</td>\n      <td>0.450157</td>\n      <td>0.927768</td>\n      <td>176.150001</td>\n      <td>85.487038</td>\n      <td>...</td>\n      <td>116072.601562</td>\n      <td>109778.625000</td>\n      <td>2.681830e+09</td>\n      <td>-1.578465</td>\n      <td>0.134022</td>\n      <td>0.446155</td>\n      <td>90220.271484</td>\n      <td>116072.593750</td>\n      <td>2.436998</td>\n      <td>0.894241</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>73.683334</td>\n      <td>24.281818</td>\n      <td>16.816668</td>\n      <td>588.533997</td>\n      <td>-0.423892</td>\n      <td>0.953657</td>\n      <td>0.952595</td>\n      <td>30.108334</td>\n      <td>21.490740</td>\n      <td>...</td>\n      <td>12793.206055</td>\n      <td>10470.508789</td>\n      <td>1.164978e+08</td>\n      <td>-1.695204</td>\n      <td>0.203371</td>\n      <td>0.843683</td>\n      <td>20261.454712</td>\n      <td>12793.206055</td>\n      <td>1.983479</td>\n      <td>0.438438</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>131.483337</td>\n      <td>42.448486</td>\n      <td>27.299999</td>\n      <td>2579.278076</td>\n      <td>-0.948834</td>\n      <td>0.820242</td>\n      <td>1.140750</td>\n      <td>70.308334</td>\n      <td>37.272224</td>\n      <td>...</td>\n      <td>48747.886719</td>\n      <td>56377.863281</td>\n      <td>9.152884e+08</td>\n      <td>-1.230018</td>\n      <td>-0.380039</td>\n      <td>0.620616</td>\n      <td>42063.210938</td>\n      <td>48747.890625</td>\n      <td>2.208131</td>\n      <td>0.535190</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>250.916672</td>\n      <td>57.416664</td>\n      <td>16.816668</td>\n      <td>5966.505371</td>\n      <td>1.531150</td>\n      <td>1.564291</td>\n      <td>1.282702</td>\n      <td>77.891667</td>\n      <td>42.296295</td>\n      <td>...</td>\n      <td>37182.328125</td>\n      <td>37570.835938</td>\n      <td>4.365628e+08</td>\n      <td>-1.241227</td>\n      <td>-0.135190</td>\n      <td>0.561936</td>\n      <td>30318.465332</td>\n      <td>37182.332031</td>\n      <td>2.315747</td>\n      <td>0.757898</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.283333</td>\n      <td>166.449997</td>\n      <td>42.993935</td>\n      <td>16.766666</td>\n      <td>2538.044434</td>\n      <td>1.227072</td>\n      <td>1.483795</td>\n      <td>1.117239</td>\n      <td>50.774999</td>\n      <td>33.577774</td>\n      <td>...</td>\n      <td>68386.632812</td>\n      <td>43126.476562</td>\n      <td>3.511676e+09</td>\n      <td>-1.304017</td>\n      <td>0.637380</td>\n      <td>0.866535</td>\n      <td>89065.209961</td>\n      <td>68386.632812</td>\n      <td>2.046365</td>\n      <td>0.625485</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>6.650000</td>\n      <td>298.216675</td>\n      <td>90.040909</td>\n      <td>61.849998</td>\n      <td>8973.646484</td>\n      <td>0.482736</td>\n      <td>1.356871</td>\n      <td>1.003109</td>\n      <td>62.099999</td>\n      <td>76.175926</td>\n      <td>...</td>\n      <td>59718.839844</td>\n      <td>26482.294922</td>\n      <td>4.067366e+09</td>\n      <td>-1.308131</td>\n      <td>0.701559</td>\n      <td>1.067936</td>\n      <td>98704.467773</td>\n      <td>59718.839844</td>\n      <td>1.760489</td>\n      <td>0.440584</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>5.550000</td>\n      <td>513.766663</td>\n      <td>88.033333</td>\n      <td>41.150002</td>\n      <td>21160.406250</td>\n      <td>5.111922</td>\n      <td>2.553533</td>\n      <td>1.575500</td>\n      <td>74.858335</td>\n      <td>49.894444</td>\n      <td>...</td>\n      <td>103515.695312</td>\n      <td>25824.949219</td>\n      <td>1.592926e+10</td>\n      <td>-1.049457</td>\n      <td>0.854721</td>\n      <td>1.219247</td>\n      <td>159958.384766</td>\n      <td>103515.695312</td>\n      <td>1.541633</td>\n      <td>0.284065</td>\n    </tr>\n    <tr>\n      <th>569</th>\n      <td>5.216667</td>\n      <td>197.933334</td>\n      <td>65.021217</td>\n      <td>36.383335</td>\n      <td>4138.562500</td>\n      <td>-0.087868</td>\n      <td>1.107714</td>\n      <td>0.943351</td>\n      <td>59.875000</td>\n      <td>56.898151</td>\n      <td>...</td>\n      <td>52158.675781</td>\n      <td>23151.261719</td>\n      <td>3.426864e+09</td>\n      <td>0.262236</td>\n      <td>1.349471</td>\n      <td>1.122333</td>\n      <td>44557.569580</td>\n      <td>52158.675781</td>\n      <td>1.827806</td>\n      <td>0.558175</td>\n    </tr>\n    <tr>\n      <th>570</th>\n      <td>5.033333</td>\n      <td>426.116669</td>\n      <td>91.619698</td>\n      <td>28.716667</td>\n      <td>16389.708984</td>\n      <td>2.290914</td>\n      <td>1.819472</td>\n      <td>1.332295</td>\n      <td>97.125001</td>\n      <td>64.074074</td>\n      <td>...</td>\n      <td>111410.710938</td>\n      <td>11569.268555</td>\n      <td>2.198547e+10</td>\n      <td>-1.314444</td>\n      <td>0.771620</td>\n      <td>1.330886</td>\n      <td>208656.183105</td>\n      <td>111410.726562</td>\n      <td>1.297107</td>\n      <td>0.216173</td>\n    </tr>\n    <tr>\n      <th>571</th>\n      <td>5.300000</td>\n      <td>127.483330</td>\n      <td>33.943935</td>\n      <td>22.883333</td>\n      <td>1269.806396</td>\n      <td>2.383691</td>\n      <td>1.772810</td>\n      <td>1.000945</td>\n      <td>31.833332</td>\n      <td>26.733334</td>\n      <td>...</td>\n      <td>9402.422852</td>\n      <td>8435.019531</td>\n      <td>5.893434e+07</td>\n      <td>-1.156760</td>\n      <td>0.447605</td>\n      <td>0.816478</td>\n      <td>10542.973938</td>\n      <td>9402.422852</td>\n      <td>2.063326</td>\n      <td>0.582836</td>\n    </tr>\n  </tbody>\n</table>\n<p>570 rows × 24 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"night\"].dropna(inplace=True)\n",
    "datasets[\"night\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:30.724879Z",
     "start_time": "2025-01-12T21:08:30.712873Z"
    }
   },
   "id": "3fe0122a03458d4e",
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "e195102c",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff79d6",
   "metadata": {},
   "source": [
    "## Classifiers, parameters, constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00c0242a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:30.731873Z",
     "start_time": "2025-01-12T21:08:30.726233Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LR\": LogisticRegression(\n",
    "        penalty=\"elasticnet\",\n",
    "        random_state=0,\n",
    "        solver=\"saga\",\n",
    "        max_iter=500\n",
    "    ),\n",
    "    \"SVM\": SVC(\n",
    "        kernel=\"rbf\",\n",
    "        cache_size=512\n",
    "    ),\n",
    "    \"RF\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        criterion=\"entropy\"\n",
    "    ),\n",
    "    \"LGBM\": LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        verbosity=-1,\n",
    "        random_state=0\n",
    "    ),\n",
    "    \"XGB\": XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=0 \n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "param_grids = {\n",
    "    \"LR\": {\n",
    "        \"C\": [0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10, 25, 50, 100, 500, 1000],\n",
    "        \"class_weight\": [\"balanced\"],\n",
    "        \"l1_ratio\": [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\n",
    "                     0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1],\n",
    "        \"max_iter\": [1000]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": [1, 10],\n",
    "        \"gamma\": [\"scale\"],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \"RF\": {\n",
    "        \"class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "    },\n",
    "    \"LGBM\": {\n",
    "        \"num_leaves\": [31, 50],\n",
    "        \"min_child_samples\": [10, 20],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \"XGB\": {\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"max_depth\": [3, 6],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.8, 1.0],\n",
    "        \"scale_pos_weight\": [1, 10]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44383487",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification - Depresjon"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebfaee915f9ed973"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6225c7bed369f0f6"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "299853bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:30.736013Z",
     "start_time": "2025-01-12T21:08:30.733043Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = DEPRESJON_PREFIX  # DEPRESJON_PREFIX or PSYKOSE_PREFIX\n",
    "y_filename = \"depresjon_y.csv\" if dataset == DEPRESJON_PREFIX else \"psykose_y.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "518e18e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T21:08:30.767791Z",
     "start_time": "2025-01-12T21:08:30.737292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming PROCESSED_DATA_DIR and y_filename are already defined\n",
    "datasets = {}\n",
    "\n",
    "# Load datasets\n",
    "for part in [\"night\"]:\n",
    "    filename = f\"{dataset}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    datasets[part] = pd.read_csv(filepath, header=0).values\n",
    "\n",
    "# Load y values\n",
    "y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, y_filename), header=None, dtype=int)\n",
    "y = y.values.ravel()\n",
    "# Usuwanie wierszy, które zawierają NaN w danych\n",
    "for part in datasets:\n",
    "    # Indeksy wierszy, które zawierają NaN w dowolnej kolumnie\n",
    "    nan_indices = np.isnan(datasets[part]).any(axis=1)\n",
    "    \n",
    "    # Usuwamy te wiersze z datasets i y\n",
    "    datasets[part] = datasets[part][~nan_indices]\n",
    "    y = y[~nan_indices]\n",
    "\n",
    "# Sprawdzamy kształt danych po usunięciu NaN\n",
    "#print(datasets['full_24h'].shape)\n",
    "#print(y.shape)\n",
    "#print(z_depresjon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb6e1ec0",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:11:41.456597Z",
     "start_time": "2025-01-12T21:08:30.788067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyniki dla Depresjon:\n",
      "PART: night\n",
      "  LR\n",
      "    accuracy: 0.6075 +- 0.0465\n",
      "    balanced_accuracy: 0.5938 +- 0.0427\n",
      "    f1: 0.5208 +- 0.0415\n",
      "    precision: 0.5320 +- 0.0675\n",
      "    recall: 0.5126 +- 0.0182\n",
      "    specificity: 0.6750 +- 0.0702\n",
      "    ROC_AUC: 0.5938 +- 0.0427\n",
      "    MCC: 0.1903 +- 0.0884\n",
      "\n",
      "  SVM\n",
      "    accuracy: 0.6258 +- 0.0632\n",
      "    balanced_accuracy: 0.6253 +- 0.0666\n",
      "    f1: 0.5791 +- 0.0805\n",
      "    precision: 0.5404 +- 0.0743\n",
      "    recall: 0.6257 +- 0.0943\n",
      "    specificity: 0.6250 +- 0.0625\n",
      "    ROC_AUC: 0.6253 +- 0.0666\n",
      "    MCC: 0.2480 +- 0.1316\n",
      "\n",
      "  RF\n",
      "    accuracy: 0.5535 +- 0.0261\n",
      "    balanced_accuracy: 0.4907 +- 0.0246\n",
      "    f1: 0.1818 +- 0.0845\n",
      "    precision: 0.3567 +- 0.0998\n",
      "    recall: 0.1251 +- 0.0669\n",
      "    specificity: 0.8562 +- 0.0375\n",
      "    ROC_AUC: 0.4907 +- 0.0246\n",
      "    MCC: -0.0345 +- 0.0760\n",
      "\n",
      "  LGBM\n",
      "    accuracy: 0.4868 +- 0.0276\n",
      "    balanced_accuracy: 0.4425 +- 0.0311\n",
      "    f1: 0.2294 +- 0.0474\n",
      "    precision: 0.3041 +- 0.0605\n",
      "    recall: 0.1851 +- 0.0398\n",
      "    specificity: 0.7000 +- 0.0424\n",
      "    ROC_AUC: 0.4425 +- 0.0311\n",
      "    MCC: -0.1299 +- 0.0670\n",
      "\n",
      "  XGB\n",
      "    accuracy: 0.5640 +- 0.0110\n",
      "    balanced_accuracy: 0.5009 +- 0.0160\n",
      "    f1: 0.3712 +- 0.3287\n",
      "    precision: 0.3267 +- 0.1718\n",
      "    recall: 0.1267 +- 0.1078\n",
      "    specificity: 0.8750 +- 0.0884\n",
      "    ROC_AUC: 0.5009 +- 0.0160\n",
      "    MCC: -0.0171 +- 0.0630\n"
     ]
    }
   ],
   "source": [
    "print(\"Wyniki dla Depresjon:\")\n",
    "for part in [\"night\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    \n",
    "    X = datasets[part]\n",
    "    for clf_type in [\"LR\", \"SVM\", \"RF\", \"LGBM\", \"XGB\"]:\n",
    "        print(f\"  {clf_type}\")\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        \n",
    "        test_scores = []\n",
    "        group_results = {}  \n",
    "        \n",
    "        for train_idx, test_idx in folds.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx] \n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.05)\n",
    "            X_train, X_test = standardize(X_train, X_test)\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"f1_weighted\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                #cv=LeaveOneOut()\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            clf = grid_search.best_estimator_\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            for i in range(len(test_idx)):\n",
    "                \n",
    "                condition_id = z_depresjon.iloc[test_idx[i]] \n",
    "                class_label = y[test_idx[i]] \n",
    "                unique_id = (condition_id, class_label)  \n",
    "\n",
    "                if unique_id not in group_results:\n",
    "                    group_results[unique_id] = []  \n",
    "                group_results[unique_id].append(y_pred[i])  \n",
    "            final_predictions = {}\n",
    "            for unique_id, preds in group_results.items():\n",
    "                condition_id, class_label = unique_id\n",
    "                majority_vote = mode(preds).mode[0]\n",
    "                final_predictions[unique_id] = majority_vote \n",
    "            y_true_grouped = []\n",
    "            y_pred_grouped =  []\n",
    "            for k,v in final_predictions.items():\n",
    "                y_true_grouped.append(k[1])\n",
    "                y_pred_grouped.append(v)\n",
    "            metrics = calculate_metrics(y_true_grouped, y_pred_grouped)\n",
    "            test_scores.append(metrics)\n",
    "            \n",
    "        final_scores = calculate_metrics_statistics(test_scores)\n",
    "        \n",
    "        for metric, (mean, stddev) in final_scores.items():\n",
    "            print(f\"    {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification - Psykose"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ba2a57410d6d575"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = PSYKOSE_PREFIX  # DEPRESJON_PREFIX or PSYKOSE_PREFIX\n",
    "y_filename = \"depresjon_y.csv\" if dataset == DEPRESJON_PREFIX else \"psykose_y.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:11:41.459604Z",
     "start_time": "2025-01-12T21:11:41.457752Z"
    }
   },
   "id": "1528ff3bdaaeb2fc",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for part in [\"night\"]:\n",
    "    filename = f\"{dataset}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    datasets[part] = pd.read_csv(filepath, header=0).values\n",
    "\n",
    "# Load y values\n",
    "y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, y_filename), header=None, dtype=int)\n",
    "y = y.values.ravel()\n",
    "# Usuwanie wierszy, które zawierają NaN w danych\n",
    "for part in datasets:\n",
    "    # Indeksy wierszy, które zawierają NaN w dowolnej kolumnie\n",
    "    nan_indices = np.isnan(datasets[part]).any(axis=1)\n",
    "    \n",
    "    # Usuwamy te wiersze z datasets i y\n",
    "    datasets[part] = datasets[part][~nan_indices]\n",
    "    y = y[~nan_indices]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T21:11:41.468126Z",
     "start_time": "2025-01-12T21:11:41.460147Z"
    }
   },
   "id": "1daca993bedf4b86",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyniki dla Psykose:\n",
      "PART: night\n",
      "  LR\n",
      "    accuracy: 0.6852 +- 0.0586\n",
      "    balanced_accuracy: 0.6918 +- 0.0644\n",
      "    f1: 0.6479 +- 0.0847\n",
      "    precision: 0.5933 +- 0.0555\n",
      "    recall: 0.7273 +- 0.1379\n",
      "    specificity: 0.6562 +- 0.0884\n",
      "    ROC_AUC: 0.6918 +- 0.0644\n",
      "    MCC: 0.3810 +- 0.1221\n",
      "\n",
      "  SVM\n",
      "    accuracy: 0.7259 +- 0.0378\n",
      "    balanced_accuracy: 0.6835 +- 0.0380\n",
      "    f1: 0.5670 +- 0.0826\n",
      "    precision: 0.8260 +- 0.1256\n",
      "    recall: 0.4545 +- 0.1185\n",
      "    specificity: 0.9125 +- 0.0996\n",
      "    ROC_AUC: 0.6835 +- 0.0380\n",
      "    MCC: 0.4413 +- 0.0867\n",
      "\n",
      "  RF\n",
      "    accuracy: 0.7148 +- 0.0222\n",
      "    balanced_accuracy: 0.6557 +- 0.0215\n",
      "    f1: 0.4894 +- 0.0402\n",
      "    precision: 0.9205 +- 0.1080\n",
      "    recall: 0.3364 +- 0.0364\n",
      "    specificity: 0.9750 +- 0.0364\n",
      "    ROC_AUC: 0.6557 +- 0.0215\n",
      "    MCC: 0.4317 +- 0.0662\n",
      "\n",
      "  LGBM\n"
     ]
    }
   ],
   "source": [
    "print(\"Wyniki dla Psykose:\")\n",
    "for part in [\"night\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    \n",
    "    X = datasets[part]\n",
    "    for clf_type in [\"LR\", \"SVM\", \"RF\", \"LGBM\", \"XGB\"]:\n",
    "        print(f\"  {clf_type}\")\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        \n",
    "        test_scores = []\n",
    "        group_results = {}  \n",
    "        \n",
    "        for train_idx, test_idx in folds.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx] \n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.05)\n",
    "            X_train, X_test = standardize(X_train, X_test)\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"f1_weighted\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                #cv=LeaveOneOut()\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            clf = grid_search.best_estimator_\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            for i in range(len(test_idx)):\n",
    "                \n",
    "                condition_id = z_psykose.iloc[test_idx[i]] \n",
    "                class_label = y[test_idx[i]] \n",
    "                unique_id = (condition_id, class_label)  \n",
    "\n",
    "                if unique_id not in group_results:\n",
    "                    group_results[unique_id] = []  \n",
    "                group_results[unique_id].append(y_pred[i])  \n",
    "            final_predictions = {}\n",
    "            for unique_id, preds in group_results.items():\n",
    "                condition_id, class_label = unique_id\n",
    "                majority_vote = mode(preds).mode[0]\n",
    "                final_predictions[unique_id] = majority_vote \n",
    "            y_true_grouped = []\n",
    "            y_pred_grouped =  []\n",
    "            for k,v in final_predictions.items():\n",
    "                y_true_grouped.append(k[1])\n",
    "                y_pred_grouped.append(v)\n",
    "            metrics = calculate_metrics(y_true_grouped, y_pred_grouped)\n",
    "            test_scores.append(metrics)\n",
    "            \n",
    "        final_scores = calculate_metrics_statistics(test_scores)\n",
    "        \n",
    "        for metric, (mean, stddev) in final_scores.items():\n",
    "            print(f\"    {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "        \n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-01-12T21:11:41.468853Z"
    }
   },
   "id": "fc396b89f05818b8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification - Hyperactive"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdfe7455e33f851c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = HYPERAKTIV_PREFIX\n",
    "y_filename = \"hyperaktiv_y.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "17343f93d6aca452",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for part in [\"night\"]:\n",
    "    filename = f\"{dataset}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    datasets[part] = pd.read_csv(filepath, header=0).values\n",
    "\n",
    "# Load y values\n",
    "y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, y_filename), header=None, dtype=int)\n",
    "y = y.values.ravel()\n",
    "# Usuwanie wierszy, które zawierają NaN w danych\n",
    "for part in datasets:\n",
    "    # Indeksy wierszy, które zawierają NaN w dowolnej kolumnie\n",
    "    nan_indices = np.isnan(datasets[part]).any(axis=1)\n",
    "    \n",
    "    # Usuwamy te wiersze z datasets i y\n",
    "    datasets[part] = datasets[part][~nan_indices]\n",
    "    y = y[~nan_indices]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f2b3643184fef0ce",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"Wyniki dla Hyperaktiv:\")\n",
    "for part in [\"night\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    \n",
    "    X = datasets[part]\n",
    "    for clf_type in [\"LR\", \"SVM\", \"RF\", \"LGBM\", \"XGB\"]:\n",
    "        print(f\"  {clf_type}\")\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        \n",
    "        test_scores = []\n",
    "        group_results = {}  \n",
    "        \n",
    "        for train_idx, test_idx in folds.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx] \n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            #X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.05)\n",
    "            X_train, X_test = standardize(X_train, X_test)\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"f1_weighted\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                #cv=LeaveOneOut()\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            clf = grid_search.best_estimator_\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            for i in range(len(test_idx)):\n",
    "                \n",
    "                condition_id = z_psykose.iloc[test_idx[i]] \n",
    "                class_label = y[test_idx[i]] \n",
    "                unique_id = (condition_id, class_label)  \n",
    "\n",
    "                if unique_id not in group_results:\n",
    "                    group_results[unique_id] = []  \n",
    "                group_results[unique_id].append(y_pred[i])  \n",
    "            final_predictions = {}\n",
    "            for unique_id, preds in group_results.items():\n",
    "                condition_id, class_label = unique_id\n",
    "                majority_vote = mode(preds).mode[0]\n",
    "                final_predictions[unique_id] = majority_vote \n",
    "            y_true_grouped = []\n",
    "            y_pred_grouped =  []\n",
    "            for k,v in final_predictions.items():\n",
    "                y_true_grouped.append(k[1])\n",
    "                y_pred_grouped.append(v)\n",
    "            metrics = calculate_metrics(y_true_grouped, y_pred_grouped)\n",
    "            test_scores.append(metrics)\n",
    "            \n",
    "        final_scores = calculate_metrics_statistics(test_scores)\n",
    "        \n",
    "        for metric, (mean, stddev) in final_scores.items():\n",
    "            print(f\"    {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "        \n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "296538fdafecbca1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5cfada277ec3c4cb",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
