{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320837f7",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18e18bae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:20:53.284462Z",
     "start_time": "2025-01-12T20:20:53.277957Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.signal\n",
    "import scipy.stats\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, LeaveOneOut\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from utils import Dataset, variance_thresholding, standardize, mcc, calculate_metrics, calculate_metrics_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b84c8514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:20:53.331767Z",
     "start_time": "2025-01-12T20:20:53.329417Z"
    }
   },
   "outputs": [],
   "source": [
    "# parameters for Welch's method for estimating power spectrum\n",
    "\n",
    "NPERSEG = 24                    # length of segment\n",
    "NOVERLAP = int(0.75 * NPERSEG)  # overlap of segments\n",
    "NFFT = NPERSEG                  # length of FFT\n",
    "WINDOW = \"hann\"                 # window function type\n",
    "\n",
    "# parameters for saving data\n",
    "PROCESSED_DATA_DIR = \"processed_data24h\"\n",
    "DEPRESJON_PREFIX = \"manual_depresjon24h\"\n",
    "HYPERAKTIV_PREFIX = \"manual_hyperaktiv24h\"\n",
    "PSYKOSE_PREFIX = \"manual_psykose24h\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef0d95",
   "metadata": {},
   "source": [
    "# Manual feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9515d10",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657c3d7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:20:53.346413Z",
     "start_time": "2025-01-12T20:20:53.333820Z"
    }
   },
   "outputs": [],
   "source": [
    "def basic_data_cleaning(data: List[pd.DataFrame]) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Assumes DataFrames with \"timestamp\", \"date\" and \"activity\" columns.\n",
    "    \n",
    "    Performs cleaning operations:\n",
    "    - assure format YYYY-MM-DD HH:MM:SS for \"timestamp\"\n",
    "    - drop redundant \"date\" column\n",
    "    - assure float32 format for \"activity\"\n",
    "    \n",
    "    :param data: list of DataFrames\n",
    "    :returns: list of cleaned DataFrames\n",
    "    \"\"\"\n",
    "    data = [df.copy() for df in data]  # create copy to avoid side effects\n",
    "    \n",
    "    for df in data:\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],\n",
    "                                         format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        df.drop(\"date\", axis=1, inplace=True)\n",
    "        df[\"activity\"] = df[\"activity\"].astype(np.float32)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_day_part(df: pd.DataFrame, part: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For given DataFrame with \"timestamp\" column returns only those rows that\n",
    "    correspond to the chosen part of day.\n",
    "    \n",
    "    Parts are \"day\" and \"night\", defined as:\n",
    "    - \"day\": [8:00, 21:00)\n",
    "    - \"night\": [21:00, 8:00)\n",
    "    \n",
    "    :param df: DataFrame to select rows from\n",
    "    :param part: part of day, either \"day\" or \"night\"\n",
    "    :returns: DataFrame, subset of rows of df\n",
    "    \"\"\"\n",
    "    if part == \"day\":\n",
    "        df = df.loc[(df[\"timestamp\"].dt.hour >= 8) &\n",
    "                    (df[\"timestamp\"].dt.hour < 21)]\n",
    "    elif part == \"night\":\n",
    "        df = df.loc[(df[\"timestamp\"].dt.hour >= 21) |\n",
    "                    (df[\"timestamp\"].dt.hour < 8)]\n",
    "    else:\n",
    "        raise ValueError(f'Part should be \"day\" or \"night\", got \"{part}\"')\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_missing_activity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Makes sure that \"timestamp\" column has minute resolution with no missing\n",
    "    values from start to end and replaces all NaNs in \"activity\" column with\n",
    "    mean average value.\n",
    "    \n",
    "    :param data: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    :returns: cleaned DataFrame\n",
    "    \"\"\"\n",
    "    df = df.copy()  # create copy to avoid side effects\n",
    "    \n",
    "    # resample to the basic frequency, i.e. minute; this will create NaNs for\n",
    "    # any rows that may be missing\n",
    "    df = df.resample(\"min\", on=\"timestamp\").mean()\n",
    "    \n",
    "    # recreate index and \"timestamp\" column\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # fill any NaNs with mean activity value\n",
    "    df[\"activity\"] = df[\"activity\"].fillna(df[\"activity\"].mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def resample(df: pd.DataFrame, freq: str = \"H\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resamples time series DataFrame with given frequency, aggregating each\n",
    "    segment with a mean.\n",
    "\n",
    "    :param df: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    :param freq: resampling frequency passed to Pandas resample() function\n",
    "    :returns: DataFrame with \"timestamp\" and \"activity\" columns\n",
    "    \"\"\"\n",
    "    df = df.copy()  # create copy to avoid side effects\n",
    "    \n",
    "    # group with given frequency\n",
    "    df = df.resample(freq, on=\"timestamp\").mean()\n",
    "\n",
    "    # recreate \"timestamp\" column\n",
    "    df = df.reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def proportion_of_zeros(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates proportion of zeros in given array, i.e. number of zeros divided\n",
    "    by length of array.\n",
    "    \n",
    "    :param x: 1D Numpy array\n",
    "    :returns: proportion of zeros\n",
    "    \"\"\"\n",
    "    # we may be dealing with floating numbers, we can't use direct comparison\n",
    "    zeros_count = np.sum(np.isclose(x, 0))\n",
    "    return zeros_count / len(x)\n",
    "\n",
    "\n",
    "def power_spectral_density(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates power spectral density (PSD) from \"activity\" column of a\n",
    "    DataFrame.\n",
    "    \n",
    "    :param df: DataFrame with \"activity\" column\n",
    "    :returns: 1D Numpy array with power spectral density\n",
    "    \"\"\"\n",
    "    psd = scipy.signal.welch(\n",
    "        x=df[\"activity\"].values,\n",
    "        fs=(1/24),\n",
    "        nperseg=11,\n",
    "        noverlap=10,\n",
    "        nfft=NFFT,\n",
    "        window=WINDOW,\n",
    "        scaling=\"density\"\n",
    "    )[1]\n",
    "    return psd\n",
    "\n",
    "\n",
    "def spectral_flatness(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculates spectral flatness of a signal, i.e. a geometric mean of the\n",
    "    power spectrum divided by the arithmetic mean of the power spectrum.\n",
    "    \n",
    "    If some frequency bins in the power spectrum are close to zero, they are\n",
    "    removed prior to calculation of spectral flatness to avoid calculation of\n",
    "    log(0).\n",
    "    \n",
    "    :param df: DataFrame with \"activity\" column\n",
    "    :returns: spectral flatness value\n",
    "    \"\"\"\n",
    "    power_spectrum = scipy.signal.welch(\n",
    "        df[\"activity\"].values,\n",
    "        fs=(1/24),\n",
    "        nperseg=11,\n",
    "        noverlap=10,\n",
    "        nfft=NFFT,\n",
    "        window=WINDOW,\n",
    "        scaling=\"spectrum\"\n",
    "    )[1]\n",
    "    \n",
    "    non_zeros_mask = ~np.isclose(power_spectrum, 0)\n",
    "    power_spectrum = power_spectrum[non_zeros_mask]\n",
    "    \n",
    "    return scipy.stats.gmean(power_spectrum) / power_spectrum.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c465a53",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16826d4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:20:53.350984Z",
     "start_time": "2025-01-12T20:20:53.347545Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts features from activity signal in time domain.\n",
    "    \n",
    "    :param df_resampled: DataFrame with \"activity\" column\n",
    "    :returns: DataFrame with a single row representing features\n",
    "    \"\"\"\n",
    "    X = df[\"activity\"].values\n",
    "    \n",
    "    features = {\n",
    "        \"minimum\": np.min(X),\n",
    "        \"maximum\": np.max(X),\n",
    "        \"mean\": np.mean(X),\n",
    "        \"median\": np.median(X),\n",
    "        \"variance\": np.var(X, ddof=1),  # apply Bessel's correction\n",
    "        \"kurtosis\": sp.stats.kurtosis(X),\n",
    "        \"skewness\": sp.stats.skew(X),\n",
    "        \"coeff_of_var\": sp.stats.variation(X),\n",
    "        \"iqr\": sp.stats.iqr(X),\n",
    "        \"trimmed_mean\": sp.stats.trim_mean(X, proportiontocut=0.1),\n",
    "        \"entropy\": sp.stats.entropy(X, base=2),\n",
    "        \"proportion_of_zeros\": proportion_of_zeros(X)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9589aaf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:20:53.354457Z",
     "start_time": "2025-01-12T20:20:53.351676Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_frequency_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extracts features from activity signal in frequency domain, i.e. calculated\n",
    "    from its Power Spectral Density (PSD).\n",
    "    \n",
    "    :param df: DataFrame with \"activity\" column\n",
    "    :returns: DataFrame with a single row representing features\n",
    "    \"\"\"\n",
    "    X = power_spectral_density(df)\n",
    "    \n",
    "    features = {\n",
    "        \"minimum\": np.min(X),\n",
    "        \"maximum\": np.max(X),\n",
    "        \"mean\": np.mean(X),\n",
    "        \"median\": np.median(X),\n",
    "        \"variance\": np.var(X),\n",
    "        \"kurtosis\": sp.stats.kurtosis(X),\n",
    "        \"skewness\": sp.stats.skew(X),\n",
    "        \"coeff_of_var\": sp.stats.variation(X),\n",
    "        \"iqr\": sp.stats.iqr(X),\n",
    "        \"trimmed_mean\": sp.stats.trim_mean(X, proportiontocut=0.1),\n",
    "        \"entropy\": sp.stats.entropy(X, base=2),\n",
    "        \"spectral_flatness\": spectral_flatness(df)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9867a83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:20:53.360167Z",
     "start_time": "2025-01-12T20:20:53.356125Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features_for_dataframes(dfs: List[pd.DataFrame], freq: str = \"H\") \\\n",
    "        -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Calculates time and frequency features for given DataFrames. Uses given\n",
    "    frequency for resampling.\n",
    "    \n",
    "    Calculates features separately for:\n",
    "    - full 24hs\n",
    "    - days: [8:00, 21:00)\n",
    "    - nights: [21:00, 8:00)\n",
    "    \n",
    "    :param dfs: list of DataFrames to extract features from; each one has to\n",
    "    have \"timestamp\" and \"activity\" columns\n",
    "    :param freq: resampling frequency\n",
    "    :returns: dictionary with keys \"full_24h\", \"day\" and \"night\", corresponding\n",
    "    to features from given parts of day\n",
    "    \"\"\"\n",
    "    full_dfs = basic_data_cleaning(dfs)\n",
    "    full_dfs = [fill_missing_activity(df) for df in full_dfs]\n",
    "    full_dfs = [resample(df, freq=freq) for df in full_dfs]\n",
    "    \n",
    "    night_dfs = [get_day_part(df, part=\"night\") for df in full_dfs]\n",
    "    day_dfs = [get_day_part(df, part=\"day\") for df in full_dfs]\n",
    "\n",
    "    datasets = {}\n",
    "    \n",
    "    for part, list_of_dfs in [(\"full_24h\", full_dfs), (\"night\", night_dfs),\n",
    "                              (\"day\", day_dfs)]:\n",
    "        features = []\n",
    "        for df in list_of_dfs:\n",
    "            time_features = extract_time_features(df)\n",
    "            freq_features = extract_frequency_features(df)\n",
    "\n",
    "            merged_features = pd.merge(\n",
    "                time_features,\n",
    "                freq_features,\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                suffixes=[\"_time\", \"_freq\"]\n",
    "            )\n",
    "            features.append(merged_features)\n",
    "\n",
    "        datasets[part] = pd.concat(features)\n",
    "        datasets[part].reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6d128",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Depresjon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "278e4de3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:20:54.460237Z",
     "start_time": "2025-01-12T20:20:53.360914Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(dirpath=os.path.join(\"data_24h\", \"depresjon\"))\n",
    "condition = dataset.condition\n",
    "control = dataset.control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0234b1a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:20:54.468765Z",
     "start_time": "2025-01-12T20:20:54.461184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                timestamp        date  activity\n0     2003-05-07 12:00:00  2003-05-07         0\n1     2003-05-07 12:01:00  2003-05-07       143\n2     2003-05-07 12:02:00  2003-05-07         0\n3     2003-05-07 12:03:00  2003-05-07        20\n4     2003-05-07 12:04:00  2003-05-07       166\n...                   ...         ...       ...\n1435  2003-05-08 11:55:00  2003-05-08       259\n1436  2003-05-08 11:56:00  2003-05-08       190\n1437  2003-05-08 11:57:00  2003-05-08       306\n1438  2003-05-08 11:58:00  2003-05-08        91\n1439  2003-05-08 11:59:00  2003-05-08       296\n\n[1440 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>date</th>\n      <th>activity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2003-05-07 12:00:00</td>\n      <td>2003-05-07</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2003-05-07 12:01:00</td>\n      <td>2003-05-07</td>\n      <td>143</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2003-05-07 12:02:00</td>\n      <td>2003-05-07</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003-05-07 12:03:00</td>\n      <td>2003-05-07</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2003-05-07 12:04:00</td>\n      <td>2003-05-07</td>\n      <td>166</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1435</th>\n      <td>2003-05-08 11:55:00</td>\n      <td>2003-05-08</td>\n      <td>259</td>\n    </tr>\n    <tr>\n      <th>1436</th>\n      <td>2003-05-08 11:56:00</td>\n      <td>2003-05-08</td>\n      <td>190</td>\n    </tr>\n    <tr>\n      <th>1437</th>\n      <td>2003-05-08 11:57:00</td>\n      <td>2003-05-08</td>\n      <td>306</td>\n    </tr>\n    <tr>\n      <th>1438</th>\n      <td>2003-05-08 11:58:00</td>\n      <td>2003-05-08</td>\n      <td>91</td>\n    </tr>\n    <tr>\n      <th>1439</th>\n      <td>2003-05-08 11:59:00</td>\n      <td>2003-05-08</td>\n      <td>296</td>\n    </tr>\n  </tbody>\n</table>\n<p>1440 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb28b3e0",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T20:21:03.090971Z",
     "start_time": "2025-01-12T20:20:54.469369Z"
    }
   },
   "outputs": [],
   "source": [
    "condition_parts_dfs = extract_features_for_dataframes(condition, freq=\"H\")\n",
    "control_parts_dfs = extract_features_for_dataframes(control, freq=\"H\")\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for part in [\"full_24h\"]:\n",
    "    condition_df = condition_parts_dfs[part]\n",
    "    control_df = control_parts_dfs[part]\n",
    "    \n",
    "    entire_df = pd.concat([condition_df, control_df], ignore_index=True)\n",
    "    \n",
    "    # Przypisujemy wynik do słownika datasets\n",
    "    datasets[part] = entire_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdbfbb10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:11.103718Z",
     "start_time": "2025-01-12T20:23:11.063865Z"
    }
   },
   "outputs": [],
   "source": [
    "for part, df in datasets.items():\n",
    "    filename = f\"{DEPRESJON_PREFIX}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "     minimum_time  maximum_time   mean_time  median_time  variance_time  \\\n0        0.966667    395.649994  131.482635   105.699997   16655.847656   \n1        2.083333    432.483337  153.253464    97.983337   20828.339844   \n2        0.583333    339.666656  124.177773    99.625000   12966.193359   \n3        0.916667    447.583344  128.460403   101.724998   14834.280273   \n4        2.666667    368.250000  102.386108    59.950001   11585.225586   \n..            ...           ...         ...          ...            ...   \n366      1.333333    635.616638  223.734741   204.725006   44670.164062   \n367      0.000000    896.299988  123.432640     0.000000   66508.078125   \n368      0.000000     12.933333    1.077083     0.000000       7.640334   \n369      0.000000      7.000000    0.408333     0.000000       2.066377   \n370      0.000000      8.800000    0.859028     0.025000       3.990894   \n\n     kurtosis_time  skewness_time  coeff_of_var_time    iqr_time  \\\n0        -0.920654       0.638463           0.960889  212.062506   \n1        -1.155962       0.531038           0.921882  247.795844   \n2        -1.028185       0.526891           0.897678  184.895827   \n3         0.110828       0.837585           0.928159  193.966663   \n4         0.219186       1.074547           1.029128  136.641668   \n..             ...            ...                ...         ...   \n366      -1.252771       0.421382           0.924770  386.612508   \n367       3.024749       2.085160           2.045340   50.679169   \n368      12.440123       3.530067           2.512264    0.487500   \n369      17.069595       4.271915           3.446262    0.012500   \n370       8.797597       3.030455           2.276597    0.537500   \n\n     trimmed_mean_time  ...      mean_freq    median_freq  variance_freq  \\\n0           120.149170  ...  145556.015625   94128.164062   1.245808e+10   \n1           142.218353  ...  225564.921875   66185.421875   7.144961e+10   \n2           115.246666  ...  337368.875000  233032.718750   9.313090e+10   \n3           114.235825  ...  293284.031250  243482.046875   2.841528e+10   \n4            88.289169  ...   75315.054688   50414.445312   2.197785e+09   \n..                 ...  ...            ...            ...            ...   \n366         208.136673  ...  502912.968750  319598.906250   1.322891e+11   \n367          64.109169  ...  643649.187500  177262.468750   6.870181e+11   \n368           0.460000  ...     387.263550     422.427948   2.624917e+04   \n369           0.080000  ...      13.137053      11.149598   1.006334e+02   \n370           0.385833  ...     163.824982     165.817337   6.205851e+03   \n\n     kurtosis_freq  skewness_freq  coeff_of_var_freq       iqr_freq  \\\n0         0.335413       1.309549           0.766824   68011.867188   \n1        -0.185189       1.162469           1.185027  291117.929688   \n2        -0.446069       0.971081           0.904569  379323.562500   \n3        -1.273162       0.381854           0.574761  266578.562500   \n4         0.339903       1.245119           0.622459   29237.988281   \n..             ...            ...                ...            ...   \n366      -1.131079       0.579551           0.723218  545265.140625   \n367      -0.371501       1.079565           1.287760  831232.497070   \n368      -0.764357      -0.678815           0.418361     234.294464   \n369      -0.397368       0.965084           0.763613       8.682478   \n370      -1.531277      -0.219038           0.480862     164.245605   \n\n     trimmed_mean_freq  entropy_freq  spectral_flatness  \n0        131411.703125      3.339275           0.781018  \n1        191007.359375      2.800678           0.442222  \n2        306780.750000      3.145580           0.615705  \n3        286987.468750      3.457804           0.826515  \n4         69791.164062      3.455590           0.847506  \n..                 ...           ...                ...  \n366      481347.125000      3.324815           0.737595  \n367      537330.375000      2.543363           0.199196  \n368         400.103149      3.546332           0.857989  \n369          12.300607      3.309888           0.739117  \n370         166.905380      3.514199           0.853088  \n\n[371 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>minimum_time</th>\n      <th>maximum_time</th>\n      <th>mean_time</th>\n      <th>median_time</th>\n      <th>variance_time</th>\n      <th>kurtosis_time</th>\n      <th>skewness_time</th>\n      <th>coeff_of_var_time</th>\n      <th>iqr_time</th>\n      <th>trimmed_mean_time</th>\n      <th>...</th>\n      <th>mean_freq</th>\n      <th>median_freq</th>\n      <th>variance_freq</th>\n      <th>kurtosis_freq</th>\n      <th>skewness_freq</th>\n      <th>coeff_of_var_freq</th>\n      <th>iqr_freq</th>\n      <th>trimmed_mean_freq</th>\n      <th>entropy_freq</th>\n      <th>spectral_flatness</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.966667</td>\n      <td>395.649994</td>\n      <td>131.482635</td>\n      <td>105.699997</td>\n      <td>16655.847656</td>\n      <td>-0.920654</td>\n      <td>0.638463</td>\n      <td>0.960889</td>\n      <td>212.062506</td>\n      <td>120.149170</td>\n      <td>...</td>\n      <td>145556.015625</td>\n      <td>94128.164062</td>\n      <td>1.245808e+10</td>\n      <td>0.335413</td>\n      <td>1.309549</td>\n      <td>0.766824</td>\n      <td>68011.867188</td>\n      <td>131411.703125</td>\n      <td>3.339275</td>\n      <td>0.781018</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.083333</td>\n      <td>432.483337</td>\n      <td>153.253464</td>\n      <td>97.983337</td>\n      <td>20828.339844</td>\n      <td>-1.155962</td>\n      <td>0.531038</td>\n      <td>0.921882</td>\n      <td>247.795844</td>\n      <td>142.218353</td>\n      <td>...</td>\n      <td>225564.921875</td>\n      <td>66185.421875</td>\n      <td>7.144961e+10</td>\n      <td>-0.185189</td>\n      <td>1.162469</td>\n      <td>1.185027</td>\n      <td>291117.929688</td>\n      <td>191007.359375</td>\n      <td>2.800678</td>\n      <td>0.442222</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.583333</td>\n      <td>339.666656</td>\n      <td>124.177773</td>\n      <td>99.625000</td>\n      <td>12966.193359</td>\n      <td>-1.028185</td>\n      <td>0.526891</td>\n      <td>0.897678</td>\n      <td>184.895827</td>\n      <td>115.246666</td>\n      <td>...</td>\n      <td>337368.875000</td>\n      <td>233032.718750</td>\n      <td>9.313090e+10</td>\n      <td>-0.446069</td>\n      <td>0.971081</td>\n      <td>0.904569</td>\n      <td>379323.562500</td>\n      <td>306780.750000</td>\n      <td>3.145580</td>\n      <td>0.615705</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.916667</td>\n      <td>447.583344</td>\n      <td>128.460403</td>\n      <td>101.724998</td>\n      <td>14834.280273</td>\n      <td>0.110828</td>\n      <td>0.837585</td>\n      <td>0.928159</td>\n      <td>193.966663</td>\n      <td>114.235825</td>\n      <td>...</td>\n      <td>293284.031250</td>\n      <td>243482.046875</td>\n      <td>2.841528e+10</td>\n      <td>-1.273162</td>\n      <td>0.381854</td>\n      <td>0.574761</td>\n      <td>266578.562500</td>\n      <td>286987.468750</td>\n      <td>3.457804</td>\n      <td>0.826515</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.666667</td>\n      <td>368.250000</td>\n      <td>102.386108</td>\n      <td>59.950001</td>\n      <td>11585.225586</td>\n      <td>0.219186</td>\n      <td>1.074547</td>\n      <td>1.029128</td>\n      <td>136.641668</td>\n      <td>88.289169</td>\n      <td>...</td>\n      <td>75315.054688</td>\n      <td>50414.445312</td>\n      <td>2.197785e+09</td>\n      <td>0.339903</td>\n      <td>1.245119</td>\n      <td>0.622459</td>\n      <td>29237.988281</td>\n      <td>69791.164062</td>\n      <td>3.455590</td>\n      <td>0.847506</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>366</th>\n      <td>1.333333</td>\n      <td>635.616638</td>\n      <td>223.734741</td>\n      <td>204.725006</td>\n      <td>44670.164062</td>\n      <td>-1.252771</td>\n      <td>0.421382</td>\n      <td>0.924770</td>\n      <td>386.612508</td>\n      <td>208.136673</td>\n      <td>...</td>\n      <td>502912.968750</td>\n      <td>319598.906250</td>\n      <td>1.322891e+11</td>\n      <td>-1.131079</td>\n      <td>0.579551</td>\n      <td>0.723218</td>\n      <td>545265.140625</td>\n      <td>481347.125000</td>\n      <td>3.324815</td>\n      <td>0.737595</td>\n    </tr>\n    <tr>\n      <th>367</th>\n      <td>0.000000</td>\n      <td>896.299988</td>\n      <td>123.432640</td>\n      <td>0.000000</td>\n      <td>66508.078125</td>\n      <td>3.024749</td>\n      <td>2.085160</td>\n      <td>2.045340</td>\n      <td>50.679169</td>\n      <td>64.109169</td>\n      <td>...</td>\n      <td>643649.187500</td>\n      <td>177262.468750</td>\n      <td>6.870181e+11</td>\n      <td>-0.371501</td>\n      <td>1.079565</td>\n      <td>1.287760</td>\n      <td>831232.497070</td>\n      <td>537330.375000</td>\n      <td>2.543363</td>\n      <td>0.199196</td>\n    </tr>\n    <tr>\n      <th>368</th>\n      <td>0.000000</td>\n      <td>12.933333</td>\n      <td>1.077083</td>\n      <td>0.000000</td>\n      <td>7.640334</td>\n      <td>12.440123</td>\n      <td>3.530067</td>\n      <td>2.512264</td>\n      <td>0.487500</td>\n      <td>0.460000</td>\n      <td>...</td>\n      <td>387.263550</td>\n      <td>422.427948</td>\n      <td>2.624917e+04</td>\n      <td>-0.764357</td>\n      <td>-0.678815</td>\n      <td>0.418361</td>\n      <td>234.294464</td>\n      <td>400.103149</td>\n      <td>3.546332</td>\n      <td>0.857989</td>\n    </tr>\n    <tr>\n      <th>369</th>\n      <td>0.000000</td>\n      <td>7.000000</td>\n      <td>0.408333</td>\n      <td>0.000000</td>\n      <td>2.066377</td>\n      <td>17.069595</td>\n      <td>4.271915</td>\n      <td>3.446262</td>\n      <td>0.012500</td>\n      <td>0.080000</td>\n      <td>...</td>\n      <td>13.137053</td>\n      <td>11.149598</td>\n      <td>1.006334e+02</td>\n      <td>-0.397368</td>\n      <td>0.965084</td>\n      <td>0.763613</td>\n      <td>8.682478</td>\n      <td>12.300607</td>\n      <td>3.309888</td>\n      <td>0.739117</td>\n    </tr>\n    <tr>\n      <th>370</th>\n      <td>0.000000</td>\n      <td>8.800000</td>\n      <td>0.859028</td>\n      <td>0.025000</td>\n      <td>3.990894</td>\n      <td>8.797597</td>\n      <td>3.030455</td>\n      <td>2.276597</td>\n      <td>0.537500</td>\n      <td>0.385833</td>\n      <td>...</td>\n      <td>163.824982</td>\n      <td>165.817337</td>\n      <td>6.205851e+03</td>\n      <td>-1.531277</td>\n      <td>-0.219038</td>\n      <td>0.480862</td>\n      <td>164.245605</td>\n      <td>166.905380</td>\n      <td>3.514199</td>\n      <td>0.853088</td>\n    </tr>\n  </tbody>\n</table>\n<p>371 rows × 24 columns</p>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:11.643361Z",
     "start_time": "2025-01-12T20:23:11.634016Z"
    }
   },
   "id": "e803b6a2e635670d",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9c20b2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:12.258345Z",
     "start_time": "2025-01-12T20:23:12.253677Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np.concatenate((np.ones(len(condition)), np.zeros(len(control))))\n",
    "y = pd.Series(y, dtype=int)\n",
    "\n",
    "filepath = os.path.join(PROCESSED_DATA_DIR, f\"depresjon_y.csv\")\n",
    "y.to_csv(filepath, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9127c366",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:12.731595Z",
     "start_time": "2025-01-12T20:23:12.729279Z"
    }
   },
   "outputs": [],
   "source": [
    "# to get ensemble:\n",
    "import os\n",
    "\n",
    "def sort_key(filename):\n",
    "    parts = filename.split(\"_\")\n",
    "    # Wydobycie numerów jako liczb całkowitych\n",
    "    condition_number = int(parts[1])  # Druga część np. \"1\" z \"condition_1\"\n",
    "    segment_number = int(parts[3].split(\".\")[0])  # Trzecia część np. \"1\" z \"segmen1.csv\"\n",
    "    return (condition_number, segment_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Removing NaNs\n",
    "folder_control = './data_24h/depresjon/control'\n",
    "folder_condition = './data_24h/depresjon/condition'\n",
    "z_control = []\n",
    "z_condition = []\n",
    "for filename in sorted(os.listdir(folder_control), key=sort_key):\n",
    "    z_control.append(filename.split(\"_\")[0] + filename.split(\"_\")[1])\n",
    "    \n",
    "for filename in sorted(os.listdir(folder_condition), key=sort_key):\n",
    "    z_condition.append(filename.split(\"_\")[0] + filename.split(\"_\")[1])\n",
    "    \n",
    "z_control = pd.Series(z_control)\n",
    "z_condition = pd.Series(z_condition)\n",
    "\n",
    "z = pd.concat([z_condition, z_control])\n",
    "z.index = datasets[\"full_24h\"].index\n",
    "\n",
    "mask = datasets[\"full_24h\"].notna().all(axis=1)\n",
    "datasets[\"full_24h\"] = datasets[\"full_24h\"][mask]\n",
    "y = y[mask]\n",
    "z_depresjon = z[mask]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:13.189102Z",
     "start_time": "2025-01-12T20:23:13.180335Z"
    }
   },
   "id": "902f2fb34966c617",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "9e10ba1f",
   "metadata": {},
   "source": [
    "## Psykose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f315198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:15.207674Z",
     "start_time": "2025-01-12T20:23:14.040514Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(dirpath=os.path.join(\"data_24h\", \"psykose\"))\n",
    "condition = dataset.condition\n",
    "control = dataset.control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5686d4e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:22.514113Z",
     "start_time": "2025-01-12T20:23:15.208634Z"
    }
   },
   "outputs": [],
   "source": [
    "condition_parts_dfs = extract_features_for_dataframes(condition, freq=\"H\")\n",
    "control_parts_dfs = extract_features_for_dataframes(control, freq=\"H\")\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for part in [\"full_24h\"]:\n",
    "    condition_df = condition_parts_dfs[part]\n",
    "    control_df = control_parts_dfs[part]\n",
    "\n",
    "    entire_df = pd.concat([condition_df, control_df], ignore_index=True)\n",
    "\n",
    "    # Przypisujemy wynik do słownika datasets\n",
    "    datasets[part] = entire_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cc17091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:22.527100Z",
     "start_time": "2025-01-12T20:23:22.514951Z"
    }
   },
   "outputs": [],
   "source": [
    "for part, df in datasets.items():\n",
    "    filename = f\"{PSYKOSE_PREFIX}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e75053c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:28.645808Z",
     "start_time": "2025-01-12T20:23:28.639755Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np.concatenate((np.ones(len(condition)), np.zeros(len(control))))\n",
    "y = pd.Series(y, dtype=int)\n",
    "\n",
    "filepath = os.path.join(PROCESSED_DATA_DIR, f\"psykose_y.csv\")\n",
    "y.to_csv(filepath, header=False, index=False)\n",
    "#datasets[\"full_24h\"].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Removing NaNs\n",
    "folder_control = './data_24h/psykose/control'\n",
    "folder_condition = './data_24h/psykose/condition'\n",
    "z_control = []\n",
    "z_condition = []\n",
    "for filename in sorted(os.listdir(folder_control), key=sort_key):\n",
    "    z_control.append(filename.split(\"_\")[0] + filename.split(\"_\")[1])\n",
    "    \n",
    "for filename in sorted(os.listdir(folder_condition), key=sort_key):\n",
    "    z_condition.append(filename.split(\"_\")[0] + filename.split(\"_\")[1])\n",
    "    \n",
    "z_control = pd.Series(z_control)\n",
    "z_condition = pd.Series(z_condition)\n",
    "\n",
    "z = pd.concat([z_condition, z_control])\n",
    "z.index = datasets[\"full_24h\"].index\n",
    "mask = datasets[\"full_24h\"].notna().all(axis=1)\n",
    "datasets[\"full_24h\"] = datasets[\"full_24h\"][mask]\n",
    "y = y[mask]\n",
    "z_psykose = z[mask]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:29.134135Z",
     "start_time": "2025-01-12T20:23:29.127337Z"
    }
   },
   "id": "5626befe714bfb4f",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "e195102c",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff79d6",
   "metadata": {},
   "source": [
    "## Classifiers, parameters, constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00c0242a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:31.111759Z",
     "start_time": "2025-01-12T20:23:31.105025Z"
    }
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LR\": LogisticRegression(\n",
    "        penalty=\"elasticnet\",\n",
    "        random_state=0,\n",
    "        solver=\"saga\",\n",
    "        max_iter=500\n",
    "    ),\n",
    "    \"SVM\": SVC(\n",
    "        kernel=\"rbf\",\n",
    "        cache_size=512\n",
    "    ),\n",
    "    \"RF\": RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        criterion=\"entropy\"\n",
    "    ),\n",
    "    \"LGBM\": LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        verbosity=-1,\n",
    "        random_state=0\n",
    "    ),\n",
    "    \"XGB\": XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        random_state=0 \n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "param_grids = {\n",
    "    \"LR\": {\n",
    "        \"C\": [0.001, 0.01, 0.1, 0.5, 1, 2, 5, 10, 25, 50, 100, 500, 1000],\n",
    "        \"class_weight\": [\"balanced\"],\n",
    "        \"l1_ratio\": [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5,\n",
    "                     0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": [1, 10],\n",
    "        \"gamma\": [\"scale\"],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \"RF\": {\n",
    "        \"class_weight\": [None, \"balanced\", \"balanced_subsample\"]\n",
    "    },\n",
    "    \"LGBM\": {\n",
    "        \"num_leaves\": [31, 50],\n",
    "        \"min_child_samples\": [10, 20],\n",
    "        \"class_weight\": [None, \"balanced\"]\n",
    "    },\n",
    "    \"XGB\": {\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"max_depth\": [3, 6],\n",
    "        \"subsample\": [0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.8, 1.0],\n",
    "        \"scale_pos_weight\": [1, 10]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44383487",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification - Depresjon"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebfaee915f9ed973"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6225c7bed369f0f6"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "299853bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:33.223960Z",
     "start_time": "2025-01-12T20:23:33.221619Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = DEPRESJON_PREFIX  # DEPRESJON_PREFIX or PSYKOSE_PREFIX\n",
    "y_filename = \"depresjon_y.csv\" if dataset == DEPRESJON_PREFIX else \"psykose_y.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "518e18e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T20:23:34.281716Z",
     "start_time": "2025-01-12T20:23:34.269328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Assuming PROCESSED_DATA_DIR and y_filename are already defined\n",
    "datasets = {}\n",
    "\n",
    "# Load datasets\n",
    "for part in [\"full_24h\"]:\n",
    "    filename = f\"{dataset}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    datasets[part] = pd.read_csv(filepath, header=0).values\n",
    "\n",
    "# Load y values\n",
    "y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, y_filename), header=None, dtype=int)\n",
    "y = y.values.ravel()\n",
    "# Usuwanie wierszy, które zawierają NaN w danych\n",
    "for part in datasets:\n",
    "    # Indeksy wierszy, które zawierają NaN w dowolnej kolumnie\n",
    "    nan_indices = np.isnan(datasets[part]).any(axis=1)\n",
    "    \n",
    "    # Usuwamy te wiersze z datasets i y\n",
    "    datasets[part] = datasets[part][~nan_indices]\n",
    "    y = y[~nan_indices]\n",
    "\n",
    "# Sprawdzamy kształt danych po usunięciu NaN\n",
    "#print(datasets['full_24h'].shape)\n",
    "#print(y.shape)\n",
    "#print(z_depresjon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb6e1ec0",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T20:25:25.878918Z",
     "start_time": "2025-01-12T20:23:38.990823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyniki dla Depresjon:\n",
      "PART: full_24h\n",
      "  LR\n",
      "    accuracy: 0.4209 +- 0.0325\n",
      "    balanced_accuracy: 0.4964 +- 0.0304\n",
      "    f1: 0.5719 +- 0.0317\n",
      "    precision: 0.4118 +- 0.0225\n",
      "    recall: 0.9366 +- 0.0620\n",
      "    specificity: 0.0563 +- 0.0415\n",
      "    ROC_AUC: 0.4964 +- 0.0304\n",
      "    MCC: 0.0008 +- 0.1136\n",
      "\n",
      "  SVM\n",
      "    accuracy: 0.6148 +- 0.0490\n",
      "    balanced_accuracy: 0.6152 +- 0.0500\n",
      "    f1: 0.5700 +- 0.0586\n",
      "    precision: 0.5301 +- 0.0593\n",
      "    recall: 0.6178 +- 0.0633\n",
      "    specificity: 0.6125 +- 0.0508\n",
      "    ROC_AUC: 0.6152 +- 0.0500\n",
      "    MCC: 0.2276 +- 0.0991\n",
      "\n",
      "  RF\n",
      "    accuracy: 0.7616 +- 0.0288\n",
      "    balanced_accuracy: 0.7193 +- 0.0361\n",
      "    f1: 0.6206 +- 0.0625\n",
      "    precision: 0.9026 +- 0.0824\n",
      "    recall: 0.4762 +- 0.0645\n",
      "    specificity: 0.9625 +- 0.0306\n",
      "    ROC_AUC: 0.7193 +- 0.0361\n",
      "    MCC: 0.5233 +- 0.0768\n",
      "\n",
      "  LGBM\n",
      "    accuracy: 0.7284 +- 0.0359\n",
      "    balanced_accuracy: 0.6933 +- 0.0328\n",
      "    f1: 0.5973 +- 0.0393\n",
      "    precision: 0.8039 +- 0.1433\n",
      "    recall: 0.4865 +- 0.0508\n",
      "    specificity: 0.9000 +- 0.0800\n",
      "    ROC_AUC: 0.6933 +- 0.0328\n",
      "    MCC: 0.4455 +- 0.0968\n",
      "\n",
      "  XGB\n",
      "    accuracy: 0.6517 +- 0.0253\n",
      "    balanced_accuracy: 0.5885 +- 0.0312\n",
      "    f1: 0.3412 +- 0.0649\n",
      "    precision: 0.8270 +- 0.1845\n",
      "    recall: 0.2207 +- 0.0527\n",
      "    specificity: 0.9563 +- 0.0468\n",
      "    ROC_AUC: 0.5885 +- 0.0312\n",
      "    MCC: 0.2825 +- 0.0967\n"
     ]
    }
   ],
   "source": [
    "print(\"Wyniki dla Depresjon:\")\n",
    "for part in [\"full_24h\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    \n",
    "    X = datasets[part]\n",
    "    for clf_type in [\"LR\", \"SVM\", \"RF\", \"LGBM\", \"XGB\"]:\n",
    "        print(f\"  {clf_type}\")\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        \n",
    "        test_scores = []\n",
    "        group_results = {}  \n",
    "        \n",
    "        for train_idx, test_idx in folds.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx] \n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.05)\n",
    "            X_train, X_test = standardize(X_train, X_test)\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"f1_weighted\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                #cv=LeaveOneOut()\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            clf = grid_search.best_estimator_\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            for i in range(len(test_idx)):\n",
    "                \n",
    "                condition_id = z_depresjon.iloc[test_idx[i]] \n",
    "                class_label = y[test_idx[i]] \n",
    "                unique_id = (condition_id, class_label)  \n",
    "\n",
    "                if unique_id not in group_results:\n",
    "                    group_results[unique_id] = []  \n",
    "                group_results[unique_id].append(y_pred[i])  \n",
    "            final_predictions = {}\n",
    "            for unique_id, preds in group_results.items():\n",
    "                condition_id, class_label = unique_id\n",
    "                majority_vote = mode(preds).mode[0]\n",
    "                final_predictions[unique_id] = majority_vote \n",
    "            y_true_grouped = []\n",
    "            y_pred_grouped =  []\n",
    "            for k,v in final_predictions.items():\n",
    "                y_true_grouped.append(k[1])\n",
    "                y_pred_grouped.append(v)\n",
    "            metrics = calculate_metrics(y_true_grouped, y_pred_grouped)\n",
    "            test_scores.append(metrics)\n",
    "            \n",
    "        final_scores = calculate_metrics_statistics(test_scores)\n",
    "        \n",
    "        for metric, (mean, stddev) in final_scores.items():\n",
    "            print(f\"    {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification - Psykose"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ba2a57410d6d575"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = PSYKOSE_PREFIX  # DEPRESJON_PREFIX or PSYKOSE_PREFIX\n",
    "y_filename = \"depresjon_y.csv\" if dataset == DEPRESJON_PREFIX else \"psykose_y.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T20:26:47.216929Z",
     "start_time": "2025-01-12T20:26:47.206847Z"
    }
   },
   "id": "1528ff3bdaaeb2fc",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for part in [\"full_24h\"]:\n",
    "    filename = f\"{dataset}_{part}.csv\"\n",
    "    filepath = os.path.join(PROCESSED_DATA_DIR, filename)\n",
    "    datasets[part] = pd.read_csv(filepath, header=0).values\n",
    "\n",
    "# Load y values\n",
    "y = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, y_filename), header=None, dtype=int)\n",
    "y = y.values.ravel()\n",
    "# Usuwanie wierszy, które zawierają NaN w danych\n",
    "for part in datasets:\n",
    "    # Indeksy wierszy, które zawierają NaN w dowolnej kolumnie\n",
    "    nan_indices = np.isnan(datasets[part]).any(axis=1)\n",
    "    \n",
    "    # Usuwamy te wiersze z datasets i y\n",
    "    datasets[part] = datasets[part][~nan_indices]\n",
    "    y = y[~nan_indices]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T20:26:47.564572Z",
     "start_time": "2025-01-12T20:26:47.555853Z"
    }
   },
   "id": "1daca993bedf4b86",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wyniki dla Psykose:\n",
      "PART: full_24h\n",
      "  LR\n",
      "    accuracy: 0.4222 +- 0.0319\n",
      "    balanced_accuracy: 0.4855 +- 0.0421\n",
      "    f1: 0.5357 +- 0.0482\n",
      "    precision: 0.3973 +- 0.0253\n",
      "    recall: 0.8273 +- 0.1233\n",
      "    specificity: 0.1437 +- 0.0643\n",
      "    ROC_AUC: 0.4855 +- 0.0421\n",
      "    MCC: -0.0284 +- 0.1119\n",
      "\n",
      "  SVM\n",
      "    accuracy: 0.6704 +- 0.0181\n",
      "    balanced_accuracy: 0.5969 +- 0.0249\n",
      "    f1: 0.3259 +- 0.0751\n",
      "    precision: 0.9750 +- 0.0500\n",
      "    recall: 0.2000 +- 0.0617\n",
      "    specificity: 0.9938 +- 0.0125\n",
      "    ROC_AUC: 0.5969 +- 0.0249\n",
      "    MCC: 0.3425 +- 0.0330\n",
      "\n",
      "  RF\n",
      "    accuracy: 0.7037 +- 0.0370\n",
      "    balanced_accuracy: 0.6477 +- 0.0427\n",
      "    f1: 0.4837 +- 0.0856\n",
      "    precision: 0.8180 +- 0.0721\n",
      "    recall: 0.3455 +- 0.0739\n",
      "    specificity: 0.9500 +- 0.0153\n",
      "    ROC_AUC: 0.6477 +- 0.0427\n",
      "    MCC: 0.3827 +- 0.0929\n",
      "\n",
      "  LGBM\n",
      "    accuracy: 0.6370 +- 0.0323\n",
      "    balanced_accuracy: 0.5602 +- 0.0336\n",
      "    f1: 0.2453 +- 0.0701\n",
      "    precision: 0.8200 +- 0.2227\n",
      "    recall: 0.1455 +- 0.0445\n",
      "    specificity: 0.9750 +- 0.0306\n",
      "    ROC_AUC: 0.5602 +- 0.0336\n",
      "    MCC: 0.2298 +- 0.1218\n",
      "\n",
      "  XGB\n",
      "    accuracy: 0.6519 +- 0.0181\n",
      "    balanced_accuracy: 0.5756 +- 0.0204\n",
      "    f1: 0.2755 +- 0.0528\n",
      "    precision: 0.9100 +- 0.1114\n",
      "    recall: 0.1636 +- 0.0364\n",
      "    specificity: 0.9875 +- 0.0153\n",
      "    ROC_AUC: 0.5756 +- 0.0204\n",
      "    MCC: 0.2845 +- 0.0612\n"
     ]
    }
   ],
   "source": [
    "print(\"Wyniki dla Psykose:\")\n",
    "for part in [\"full_24h\"]:\n",
    "    print(f\"PART: {part}\")\n",
    "    \n",
    "    X = datasets[part]\n",
    "    for clf_type in [\"LR\", \"SVM\", \"RF\", \"LGBM\", \"XGB\"]:\n",
    "        print(f\"  {clf_type}\")\n",
    "        folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        \n",
    "        test_scores = []\n",
    "        group_results = {}  \n",
    "        \n",
    "        for train_idx, test_idx in folds.split(X, y):\n",
    "            X_train, X_test = X[train_idx], X[test_idx] \n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            X_train, X_test = variance_thresholding(X_train, X_test, threshold=0.05)\n",
    "            X_train, X_test = standardize(X_train, X_test)\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=classifiers[clf_type], \n",
    "                param_grid=param_grids[clf_type], \n",
    "                scoring=\"f1_weighted\",\n",
    "                n_jobs=-1,\n",
    "                refit=True,\n",
    "                #cv=LeaveOneOut()\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            clf = grid_search.best_estimator_\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            for i in range(len(test_idx)):\n",
    "                \n",
    "                condition_id = z_psykose.iloc[test_idx[i]] \n",
    "                class_label = y[test_idx[i]] \n",
    "                unique_id = (condition_id, class_label)  \n",
    "\n",
    "                if unique_id not in group_results:\n",
    "                    group_results[unique_id] = []  \n",
    "                group_results[unique_id].append(y_pred[i])  \n",
    "            final_predictions = {}\n",
    "            for unique_id, preds in group_results.items():\n",
    "                condition_id, class_label = unique_id\n",
    "                majority_vote = mode(preds).mode[0]\n",
    "                final_predictions[unique_id] = majority_vote \n",
    "            y_true_grouped = []\n",
    "            y_pred_grouped =  []\n",
    "            for k,v in final_predictions.items():\n",
    "                y_true_grouped.append(k[1])\n",
    "                y_pred_grouped.append(v)\n",
    "            metrics = calculate_metrics(y_true_grouped, y_pred_grouped)\n",
    "            test_scores.append(metrics)\n",
    "            \n",
    "        final_scores = calculate_metrics_statistics(test_scores)\n",
    "        \n",
    "        for metric, (mean, stddev) in final_scores.items():\n",
    "            print(f\"    {metric}: {mean:.4f} +- {stddev:.4f}\")\n",
    "        \n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-12T20:29:11.602580Z",
     "start_time": "2025-01-12T20:27:28.487598Z"
    }
   },
   "id": "fc396b89f05818b8",
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classification - Hyperactive"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdfe7455e33f851c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "17343f93d6aca452"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
